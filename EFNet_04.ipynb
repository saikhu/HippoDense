{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D classification example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D classification task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to classify MR images into male/female.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_classification/torch/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msaikhu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/HippoDense/wandb/run-20220607_011731-1zcwfinr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/saikhu/HippoDense/runs/1zcwfinr\" target=\"_blank\">eternal-monkey-2</a></strong> to <a href=\"https://wandb.ai/saikhu/HippoDense\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/saikhu/HippoDense/runs/1zcwfinr?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7effcfefd670>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"HippoDense\", entity=\"saikhu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1+253.ge8d2d4f4\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.12.0a0+bd13bc6\n",
      "MONAI flags: HAS_EXT = True, USE_COMPILED = False\n",
      "MONAI rev id: e8d2d4f4018217347dc41b8b87eb13bfe75bf3be\n",
      "MONAI __file__: /opt/monai/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.8\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.0.1\n",
      "Tensorboard version: 2.8.0\n",
      "gdown version: 4.4.0\n",
      "TorchVision version: 0.13.0a0\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: 1.3.0\n",
      "psutil version: 5.9.0\n",
      "pandas version: 1.3.5\n",
      "einops version: 0.4.1\n",
      "transformers version: 4.19.1\n",
      "mlflow version: 1.25.1\n",
      "pynrrd version: 0.4.3\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    EnsureType\n",
    ")\n",
    "import pickle\n",
    "import time\n",
    "from torch import nn\n",
    "pin_memory = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device.\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: images cannot be used.\n",
      "364: total images prepared with 4 different classes\n"
     ]
    }
   ],
   "source": [
    "# set this in your environment or previous cell to wherever IXI is downloaded and extracted\n",
    "modelName = 'EfficientNet_04'\n",
    "RunFolder = os.path.join(\"/HippoDense/\", modelName)\n",
    "DatasetDir = \"/Datasets/01_P_Classification_all_hippo\"\n",
    "labels = []\n",
    "images = []\n",
    "NotFitImages = []\n",
    "for root, dirs, files in os.walk(os.path.abspath(DatasetDir)):\n",
    "    for file in files:\n",
    "        if file.split(\"_\")[-3] == \"L\":\n",
    "            labels.append(2)\n",
    "            images.append(os.path.join(root, file))\n",
    "        elif file.split(\"_\")[-3] == \"E\":\n",
    "            labels.append(1)\n",
    "            images.append(os.path.join(root, file))\n",
    "        elif file.split(\"_\")[-2] == \"Normal\":\n",
    "            labels.append(3)\n",
    "            images.append(os.path.join(root, file))\n",
    "        elif file.split(\"_\")[-2] == \"Dementia\":\n",
    "            labels.append(0)\n",
    "            images.append(os.path.join(root, file))\n",
    "        else:\n",
    "            NotFitImages.append(file)\n",
    "            # print(file)\n",
    "print(\"{}: images cannot be used.\".format(len(NotFitImages)))\n",
    "# # labels = np.load(\n",
    "#     '/workspace/monai/MONAI-tutorials/3d_classification/01_P_Classification_all_labels.npy')\n",
    "NumClasses = len(np.unique(labels))\n",
    "# ClassLabels = np.array(Labels)\n",
    "if len(images)==len(labels):\n",
    "    print(\"{}: total images prepared with {} different classes\".format(len(images), NumClasses))\n",
    "\n",
    "# 2 binary labels for gender classification: man or woman\n",
    "# labels = np.array([0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Represent labels in one-hot format for binary classifier training,\n",
    "# BCEWithLogitsLoss requires target to have same shape as input\n",
    "labels = torch.nn.functional.one_hot(torch.as_tensor(labels)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Datasets/01_P_Classification_all_hippo/01_P_1744_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2125_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1974_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0032_1_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1678_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0068_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1855_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1947_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1673_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1916_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1857_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2031_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2064_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2130_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1791_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1827_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2136_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2005_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1834_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1838_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0096_3_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2009_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1956_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0497_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2101_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2164_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1693_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1726_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1691_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2015_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2121_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2037_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0068_3_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1805_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1914_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1928_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2002_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2034_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1870_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2008_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0758_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0007_1_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0467_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2106_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2046_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1756_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0364_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2019_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2071_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0297_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2026_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1919_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1734_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2080_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0165_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0004_1_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1897_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1464_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2148_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1994_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2077_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1979_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0209_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2060_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0189_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2035_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2139_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1687_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1969_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1683_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0096_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2087_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1670_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0450_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0029_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1862_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1818_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1853_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1902_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2042_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2155_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0768_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1402_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0448_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0175_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1714_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1833_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0124_6_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0397_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1730_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1672_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1107_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0803_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2049_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1815_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0668_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1984_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1877_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0076_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0332_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1831_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1970_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2012_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2014_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2040_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0407_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0161_2_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1588_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1677_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0355_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0005_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1910_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2096_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2114_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2107_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1730_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1757_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2010_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2105_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2160_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1905_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1023_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2007_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1924_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2112_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0192_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2092_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2120_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2146_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1920_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1903_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2051_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1837_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2061_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0051_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2004_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0173_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0034_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1606_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2118_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1508_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1883_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1997_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2157_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2093_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1803_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1907_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2082_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1933_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2081_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1971_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1908_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2033_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1766_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2056_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0276_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2074_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1735_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2030_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2141_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2024_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2149_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1894_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1772_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1872_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0814_1_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1939_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1976_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1817_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2088_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1404_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0191_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1957_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0031_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0248_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1884_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1701_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2017_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0457_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0198_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1598_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1930_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2158_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1995_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1852_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1732_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0267_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0829_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1992_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1958_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0188_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0017_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1964_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0159_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2044_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1755_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1869_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1959_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1715_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1832_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1754_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1733_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1736_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0096_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1942_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0657_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1749_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1926_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1991_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2128_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2115_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1868_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1579_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0457_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0278_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0255_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0065_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2127_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1491_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1881_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2095_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1856_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1727_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2021_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2001_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2150_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0331_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1840_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1962_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1990_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0138_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1925_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0154_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1835_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0155_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1751_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2073_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1821_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2143_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2066_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0308_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1790_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1847_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2094_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2156_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2053_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1753_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0185_1_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1731_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1909_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1587_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0100_1_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1809_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2138_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0013_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0056_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1989_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0077_2_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2124_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1904_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2131_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1713_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2091_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1700_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0313_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1887_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0252_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1400_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1851_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0159_2_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1886_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1801_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1980_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0006_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1675_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2110_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2062_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1880_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1722_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1699_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1482_E_MCI_hippo.nii tensor([0., 1., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1850_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2011_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0398_2_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1929_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1891_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2000_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1774_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1816_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2072_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2152_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0019_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2029_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2117_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0481_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0256_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2055_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2043_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1813_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0255_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2013_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0051_1_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1210_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2052_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1799_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1981_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1906_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1885_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1960_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1748_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0068_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0024_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1737_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0166_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1899_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0267_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2025_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1702_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0141_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2063_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1888_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2097_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1738_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2147_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1729_2_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0301_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1901_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1874_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1861_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0075_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1993_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2048_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1895_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1684_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1759_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1953_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1709_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2047_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1791_1_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0169_2_Dementia_hippo.nii tensor([1., 0., 0., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1918_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2100_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0063_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1819_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0429_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0155_2_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2020_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2068_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2145_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1797_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2070_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_2089_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0769_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1746_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0010_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1828_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0247_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1966_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0776_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1830_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_0170_1_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1911_Normal_hippo.nii tensor([0., 0., 0., 1.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1983_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n",
      "/Datasets/01_P_Classification_all_hippo/01_P_1951_L_MCI_hippo.nii tensor([0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "for i , j in zip(images, labels):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([3, 1, 224, 224, 224]) tensor([[0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((224, 224, 224)), RandRotate90(), EnsureType()])\n",
    "\n",
    "val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((224, 224, 224)), EnsureType()])\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(image_files=images, labels=labels, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=2, pin_memory=pin_memory)\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)\n",
    "batch_size = 1\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(image_files=images[:300], labels=labels[:300], transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=pin_memory)\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(image_files=images[300:], labels=labels[300:], transform=val_transforms)\n",
    "# val_loader = DataLoader(val_ds, batch_size=4, num_workers=4, pin_memory=pin_memory)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=2, pin_memory=pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/150\n",
      "1/300, train_loss: 0.4028\n",
      "2/300, train_loss: 6.4652\n",
      "3/300, train_loss: 6.5435\n",
      "4/300, train_loss: 4.9519\n",
      "5/300, train_loss: 7.4345\n",
      "6/300, train_loss: 6.2239\n",
      "7/300, train_loss: 3.5115\n",
      "8/300, train_loss: 5.8857\n",
      "9/300, train_loss: 6.5669\n",
      "10/300, train_loss: 1.6759\n",
      "11/300, train_loss: 7.8864\n",
      "12/300, train_loss: 4.1246\n",
      "13/300, train_loss: 1.5739\n",
      "14/300, train_loss: 1.0233\n",
      "15/300, train_loss: 4.5999\n",
      "16/300, train_loss: 3.1115\n",
      "17/300, train_loss: 8.7257\n",
      "18/300, train_loss: 3.8514\n",
      "19/300, train_loss: 3.1573\n",
      "20/300, train_loss: 5.7080\n",
      "21/300, train_loss: 1.9355\n",
      "22/300, train_loss: 1.9715\n",
      "23/300, train_loss: 4.6948\n",
      "24/300, train_loss: 1.2961\n",
      "25/300, train_loss: 3.4356\n",
      "26/300, train_loss: 4.4357\n",
      "27/300, train_loss: 5.7245\n",
      "28/300, train_loss: 5.2892\n",
      "29/300, train_loss: 5.8369\n",
      "30/300, train_loss: 6.0851\n",
      "31/300, train_loss: 5.7172\n",
      "32/300, train_loss: 0.6259\n",
      "33/300, train_loss: 6.6598\n",
      "34/300, train_loss: 2.7930\n",
      "35/300, train_loss: 6.1087\n",
      "36/300, train_loss: 7.3165\n",
      "37/300, train_loss: 0.6070\n",
      "38/300, train_loss: 2.0581\n",
      "39/300, train_loss: 3.9121\n",
      "40/300, train_loss: 4.0899\n",
      "41/300, train_loss: 1.0933\n",
      "42/300, train_loss: 3.6300\n",
      "43/300, train_loss: 6.7847\n",
      "44/300, train_loss: 6.9152\n",
      "45/300, train_loss: 3.7622\n",
      "46/300, train_loss: 3.5195\n",
      "47/300, train_loss: 4.9975\n",
      "48/300, train_loss: 1.1710\n",
      "49/300, train_loss: 4.2322\n",
      "50/300, train_loss: 3.7206\n",
      "51/300, train_loss: 5.7762\n",
      "52/300, train_loss: 0.9404\n",
      "53/300, train_loss: 2.2723\n",
      "54/300, train_loss: 1.1100\n",
      "55/300, train_loss: 4.4467\n",
      "56/300, train_loss: 4.7121\n",
      "57/300, train_loss: 0.2958\n",
      "58/300, train_loss: 1.9218\n",
      "59/300, train_loss: 6.4257\n",
      "60/300, train_loss: 6.4496\n",
      "61/300, train_loss: 4.3884\n",
      "62/300, train_loss: 3.1624\n",
      "63/300, train_loss: 6.0960\n",
      "64/300, train_loss: 1.4446\n",
      "65/300, train_loss: 1.3959\n",
      "66/300, train_loss: 3.0958\n",
      "67/300, train_loss: 1.0248\n",
      "68/300, train_loss: 0.8141\n",
      "69/300, train_loss: 6.9837\n",
      "70/300, train_loss: 4.7976\n",
      "71/300, train_loss: 2.2828\n",
      "72/300, train_loss: 2.2006\n",
      "73/300, train_loss: 2.1377\n",
      "74/300, train_loss: 6.6406\n",
      "75/300, train_loss: 4.5981\n",
      "76/300, train_loss: 0.6033\n",
      "77/300, train_loss: 4.7357\n",
      "78/300, train_loss: 8.9807\n",
      "79/300, train_loss: 8.3133\n",
      "80/300, train_loss: 0.0954\n",
      "81/300, train_loss: 3.3812\n",
      "82/300, train_loss: 2.7147\n",
      "83/300, train_loss: 2.7956\n",
      "84/300, train_loss: 3.6843\n",
      "85/300, train_loss: 2.9308\n",
      "86/300, train_loss: 6.2542\n",
      "87/300, train_loss: 4.3206\n",
      "88/300, train_loss: 4.1645\n",
      "89/300, train_loss: 6.7324\n",
      "90/300, train_loss: 1.4305\n",
      "91/300, train_loss: 4.9135\n",
      "92/300, train_loss: 8.6799\n",
      "93/300, train_loss: 4.5513\n",
      "94/300, train_loss: 7.8261\n",
      "95/300, train_loss: 0.0451\n",
      "96/300, train_loss: 8.4978\n",
      "97/300, train_loss: 2.5188\n",
      "98/300, train_loss: 6.7150\n",
      "99/300, train_loss: 7.5048\n",
      "100/300, train_loss: 1.0728\n",
      "101/300, train_loss: 5.5202\n",
      "102/300, train_loss: 4.6363\n",
      "103/300, train_loss: 6.6628\n",
      "104/300, train_loss: 7.8161\n",
      "105/300, train_loss: 1.3668\n",
      "106/300, train_loss: 2.6905\n",
      "107/300, train_loss: 1.9520\n",
      "108/300, train_loss: 10.6660\n",
      "109/300, train_loss: 3.8168\n",
      "110/300, train_loss: 1.6802\n",
      "111/300, train_loss: 7.3685\n",
      "112/300, train_loss: 5.6170\n",
      "113/300, train_loss: 10.1676\n",
      "114/300, train_loss: 3.8054\n",
      "115/300, train_loss: 4.2655\n",
      "116/300, train_loss: 2.5155\n",
      "117/300, train_loss: 2.7610\n",
      "118/300, train_loss: 6.4189\n",
      "119/300, train_loss: 3.8581\n",
      "120/300, train_loss: 7.1840\n",
      "121/300, train_loss: 0.2584\n",
      "122/300, train_loss: 5.8135\n",
      "123/300, train_loss: 2.1690\n",
      "124/300, train_loss: 5.7168\n",
      "125/300, train_loss: 3.0548\n",
      "126/300, train_loss: 5.0932\n",
      "127/300, train_loss: 6.4274\n",
      "128/300, train_loss: 3.3179\n",
      "129/300, train_loss: 5.1768\n",
      "130/300, train_loss: 2.2401\n",
      "131/300, train_loss: 4.7572\n",
      "132/300, train_loss: 5.5623\n",
      "133/300, train_loss: 3.2983\n",
      "134/300, train_loss: 1.0347\n",
      "135/300, train_loss: 6.0259\n",
      "136/300, train_loss: 2.7868\n",
      "137/300, train_loss: 0.8706\n",
      "138/300, train_loss: 7.1507\n",
      "139/300, train_loss: 2.9229\n",
      "140/300, train_loss: 4.2216\n",
      "141/300, train_loss: 4.1923\n",
      "142/300, train_loss: 2.8285\n",
      "143/300, train_loss: 1.7350\n",
      "144/300, train_loss: 3.2008\n",
      "145/300, train_loss: 4.3166\n",
      "146/300, train_loss: 2.7005\n",
      "147/300, train_loss: 5.6216\n",
      "148/300, train_loss: 1.1590\n",
      "149/300, train_loss: 3.4450\n",
      "150/300, train_loss: 7.3518\n",
      "151/300, train_loss: 3.6444\n",
      "152/300, train_loss: 3.4255\n",
      "153/300, train_loss: 4.0969\n",
      "154/300, train_loss: 2.5654\n",
      "155/300, train_loss: 7.6457\n",
      "156/300, train_loss: 1.1839\n",
      "157/300, train_loss: 6.7291\n",
      "158/300, train_loss: 5.1237\n",
      "159/300, train_loss: 1.7323\n",
      "160/300, train_loss: 2.0344\n",
      "161/300, train_loss: 3.0089\n",
      "162/300, train_loss: 5.5017\n",
      "163/300, train_loss: 4.7272\n",
      "164/300, train_loss: 4.1103\n",
      "165/300, train_loss: 9.4391\n",
      "166/300, train_loss: 8.9225\n",
      "167/300, train_loss: 5.5347\n",
      "168/300, train_loss: 4.0314\n",
      "169/300, train_loss: 8.7920\n",
      "170/300, train_loss: 4.9917\n",
      "171/300, train_loss: 10.3813\n",
      "172/300, train_loss: 4.2342\n",
      "173/300, train_loss: 4.3784\n",
      "174/300, train_loss: 3.4545\n",
      "175/300, train_loss: 2.5318\n",
      "176/300, train_loss: 3.0598\n",
      "177/300, train_loss: 4.3950\n",
      "178/300, train_loss: 2.6283\n",
      "179/300, train_loss: 1.8041\n",
      "180/300, train_loss: 0.8731\n",
      "181/300, train_loss: 6.4514\n",
      "182/300, train_loss: 7.3278\n",
      "183/300, train_loss: 0.5991\n",
      "184/300, train_loss: 0.5518\n",
      "185/300, train_loss: 2.5351\n",
      "186/300, train_loss: 5.6104\n",
      "187/300, train_loss: 1.4906\n",
      "188/300, train_loss: 3.1962\n",
      "189/300, train_loss: 5.6403\n",
      "190/300, train_loss: 4.9621\n",
      "191/300, train_loss: 3.3925\n",
      "192/300, train_loss: 9.3677\n",
      "193/300, train_loss: 9.5441\n",
      "194/300, train_loss: 1.1233\n",
      "195/300, train_loss: 7.2563\n",
      "196/300, train_loss: 2.6644\n",
      "197/300, train_loss: 7.2570\n",
      "198/300, train_loss: 0.2026\n",
      "199/300, train_loss: 3.5042\n",
      "200/300, train_loss: 3.4090\n",
      "201/300, train_loss: 0.7491\n",
      "202/300, train_loss: 3.6038\n",
      "203/300, train_loss: 1.6641\n",
      "204/300, train_loss: 8.4501\n",
      "205/300, train_loss: 3.0869\n",
      "206/300, train_loss: 6.5963\n",
      "207/300, train_loss: 4.1134\n",
      "208/300, train_loss: 2.4105\n",
      "209/300, train_loss: 1.9956\n",
      "210/300, train_loss: 5.9079\n",
      "211/300, train_loss: 0.7171\n",
      "212/300, train_loss: 1.5650\n",
      "213/300, train_loss: 3.5646\n",
      "214/300, train_loss: 4.3817\n",
      "215/300, train_loss: 2.4403\n",
      "216/300, train_loss: 4.9332\n",
      "217/300, train_loss: 10.4107\n",
      "218/300, train_loss: 1.6320\n",
      "219/300, train_loss: 7.7949\n",
      "220/300, train_loss: 0.5016\n",
      "221/300, train_loss: 6.1612\n",
      "222/300, train_loss: 2.7350\n",
      "223/300, train_loss: 1.6686\n",
      "224/300, train_loss: 0.2775\n",
      "225/300, train_loss: 1.3629\n",
      "226/300, train_loss: 5.9070\n",
      "227/300, train_loss: 1.3972\n",
      "228/300, train_loss: 5.8730\n",
      "229/300, train_loss: 2.4308\n",
      "230/300, train_loss: 6.6590\n",
      "231/300, train_loss: 5.3162\n",
      "232/300, train_loss: 0.0131\n",
      "233/300, train_loss: 3.2949\n",
      "234/300, train_loss: 4.8915\n",
      "235/300, train_loss: 0.2490\n",
      "236/300, train_loss: 6.5301\n",
      "237/300, train_loss: 2.8388\n",
      "238/300, train_loss: 7.8190\n",
      "239/300, train_loss: 1.4604\n",
      "240/300, train_loss: 4.2703\n",
      "241/300, train_loss: 3.9553\n",
      "242/300, train_loss: 1.2026\n",
      "243/300, train_loss: 1.1606\n",
      "244/300, train_loss: 0.6377\n",
      "245/300, train_loss: 7.5062\n",
      "246/300, train_loss: 5.6394\n",
      "247/300, train_loss: 6.3916\n",
      "248/300, train_loss: 4.7484\n",
      "249/300, train_loss: 4.3087\n",
      "250/300, train_loss: 4.0787\n",
      "251/300, train_loss: 5.7467\n",
      "252/300, train_loss: 4.1468\n",
      "253/300, train_loss: 2.9128\n",
      "254/300, train_loss: 3.9295\n",
      "255/300, train_loss: 9.2353\n",
      "256/300, train_loss: 5.8877\n",
      "257/300, train_loss: 4.7344\n",
      "258/300, train_loss: 2.6041\n",
      "259/300, train_loss: 6.8599\n",
      "260/300, train_loss: 2.5426\n",
      "261/300, train_loss: 3.2158\n",
      "262/300, train_loss: 2.3084\n",
      "263/300, train_loss: 1.6928\n",
      "264/300, train_loss: 1.4420\n",
      "265/300, train_loss: 4.2242\n",
      "266/300, train_loss: 0.2204\n",
      "267/300, train_loss: 2.9310\n",
      "268/300, train_loss: 4.1112\n",
      "269/300, train_loss: 0.4634\n",
      "270/300, train_loss: 3.6117\n",
      "271/300, train_loss: 4.0554\n",
      "272/300, train_loss: 3.9300\n",
      "273/300, train_loss: 3.4463\n",
      "274/300, train_loss: 1.3255\n",
      "275/300, train_loss: 1.6774\n",
      "276/300, train_loss: 3.3683\n",
      "277/300, train_loss: 5.1274\n",
      "278/300, train_loss: 2.6176\n",
      "279/300, train_loss: 1.5735\n",
      "280/300, train_loss: 2.2163\n",
      "281/300, train_loss: 2.7843\n",
      "282/300, train_loss: 6.4840\n",
      "283/300, train_loss: 0.1230\n",
      "284/300, train_loss: 2.9713\n",
      "285/300, train_loss: 8.9592\n",
      "286/300, train_loss: 3.3363\n",
      "287/300, train_loss: 1.2253\n",
      "288/300, train_loss: 3.5402\n",
      "289/300, train_loss: 1.8905\n",
      "290/300, train_loss: 7.2648\n",
      "291/300, train_loss: 1.2573\n",
      "292/300, train_loss: 5.0237\n",
      "293/300, train_loss: 1.6964\n",
      "294/300, train_loss: 4.1406\n",
      "295/300, train_loss: 1.0852\n",
      "296/300, train_loss: 1.3424\n",
      "297/300, train_loss: 2.6253\n",
      "298/300, train_loss: 2.1988\n",
      "299/300, train_loss: 2.0063\n",
      "300/300, train_loss: 7.7543\n",
      "epoch 1 average loss: 4.0206\n",
      "----------\n",
      "epoch 2/150\n",
      "1/300, train_loss: 2.7225\n",
      "2/300, train_loss: 4.7777\n",
      "3/300, train_loss: 2.2000\n",
      "4/300, train_loss: 4.5911\n",
      "5/300, train_loss: 6.1597\n",
      "6/300, train_loss: 2.4293\n",
      "7/300, train_loss: 4.5922\n",
      "8/300, train_loss: 2.4753\n",
      "9/300, train_loss: 4.2890\n",
      "10/300, train_loss: 2.2840\n",
      "11/300, train_loss: 8.9412\n",
      "12/300, train_loss: 8.5174\n",
      "13/300, train_loss: 7.5253\n",
      "14/300, train_loss: 3.3283\n",
      "15/300, train_loss: 0.9788\n",
      "16/300, train_loss: 8.9472\n",
      "17/300, train_loss: 4.6040\n",
      "18/300, train_loss: 1.2643\n",
      "19/300, train_loss: 3.7967\n",
      "20/300, train_loss: 0.7506\n",
      "21/300, train_loss: 1.5605\n",
      "22/300, train_loss: 0.8400\n",
      "23/300, train_loss: 4.1553\n",
      "24/300, train_loss: 3.3601\n",
      "25/300, train_loss: 5.7083\n",
      "26/300, train_loss: 2.7520\n",
      "27/300, train_loss: 1.4918\n",
      "28/300, train_loss: 1.6170\n",
      "29/300, train_loss: 3.9621\n",
      "30/300, train_loss: 5.0098\n",
      "31/300, train_loss: 3.6706\n",
      "32/300, train_loss: 5.3089\n",
      "33/300, train_loss: 8.2492\n",
      "34/300, train_loss: 5.2887\n",
      "35/300, train_loss: 0.9229\n",
      "36/300, train_loss: 0.0937\n",
      "37/300, train_loss: 1.8937\n",
      "38/300, train_loss: 2.5254\n",
      "39/300, train_loss: 2.2050\n",
      "40/300, train_loss: 4.0651\n",
      "41/300, train_loss: 6.6208\n",
      "42/300, train_loss: 2.2123\n",
      "43/300, train_loss: 5.2178\n",
      "44/300, train_loss: 3.9763\n",
      "45/300, train_loss: 2.8978\n",
      "46/300, train_loss: 3.2976\n",
      "47/300, train_loss: 2.5600\n",
      "48/300, train_loss: 8.8555\n",
      "49/300, train_loss: 4.1474\n",
      "50/300, train_loss: 3.5753\n",
      "51/300, train_loss: 4.3626\n",
      "52/300, train_loss: 7.1414\n",
      "53/300, train_loss: 4.6639\n",
      "54/300, train_loss: 1.3588\n",
      "55/300, train_loss: 0.3830\n",
      "56/300, train_loss: 2.5571\n",
      "57/300, train_loss: 0.0060\n",
      "58/300, train_loss: 5.4004\n",
      "59/300, train_loss: 4.4445\n",
      "60/300, train_loss: 4.1484\n",
      "61/300, train_loss: 4.8916\n",
      "62/300, train_loss: 2.3580\n",
      "63/300, train_loss: 1.2950\n",
      "64/300, train_loss: 1.0304\n",
      "65/300, train_loss: 7.1595\n",
      "66/300, train_loss: 3.6131\n",
      "67/300, train_loss: 5.8968\n",
      "68/300, train_loss: 4.0457\n",
      "69/300, train_loss: 1.7997\n",
      "70/300, train_loss: 1.8104\n",
      "71/300, train_loss: 1.8308\n",
      "72/300, train_loss: 6.6245\n",
      "73/300, train_loss: 3.7807\n",
      "74/300, train_loss: 5.1038\n",
      "75/300, train_loss: 2.4528\n",
      "76/300, train_loss: 5.8459\n",
      "77/300, train_loss: 2.6092\n",
      "78/300, train_loss: 1.3478\n",
      "79/300, train_loss: 0.4111\n",
      "80/300, train_loss: 1.1143\n",
      "81/300, train_loss: 6.2366\n",
      "82/300, train_loss: 0.6728\n",
      "83/300, train_loss: 6.8972\n",
      "84/300, train_loss: 1.7623\n",
      "85/300, train_loss: 7.6248\n",
      "86/300, train_loss: 0.7534\n",
      "87/300, train_loss: 3.5730\n",
      "88/300, train_loss: 3.9749\n",
      "89/300, train_loss: 2.9248\n",
      "90/300, train_loss: 6.0798\n",
      "91/300, train_loss: 3.0155\n",
      "92/300, train_loss: 2.8871\n",
      "93/300, train_loss: 3.5381\n",
      "94/300, train_loss: 5.5534\n",
      "95/300, train_loss: 1.2431\n",
      "96/300, train_loss: 2.6933\n",
      "97/300, train_loss: 1.5044\n",
      "98/300, train_loss: 8.4282\n",
      "99/300, train_loss: 8.5612\n",
      "100/300, train_loss: 3.8176\n",
      "101/300, train_loss: 6.9215\n",
      "102/300, train_loss: 3.4087\n",
      "103/300, train_loss: 7.6020\n",
      "104/300, train_loss: 1.1393\n",
      "105/300, train_loss: 9.1040\n",
      "106/300, train_loss: 3.3032\n",
      "107/300, train_loss: 3.5073\n",
      "108/300, train_loss: 2.5461\n",
      "109/300, train_loss: 0.2007\n",
      "110/300, train_loss: 5.6803\n",
      "111/300, train_loss: 6.8080\n",
      "112/300, train_loss: 3.3067\n",
      "113/300, train_loss: 2.5614\n",
      "114/300, train_loss: 1.6446\n",
      "115/300, train_loss: 3.4542\n",
      "116/300, train_loss: 7.9422\n",
      "117/300, train_loss: 1.6284\n",
      "118/300, train_loss: 7.6386\n",
      "119/300, train_loss: 3.5121\n",
      "120/300, train_loss: 0.7243\n",
      "121/300, train_loss: 0.2327\n",
      "122/300, train_loss: 5.5234\n",
      "123/300, train_loss: 4.2342\n",
      "124/300, train_loss: 5.0870\n",
      "125/300, train_loss: 2.0359\n",
      "126/300, train_loss: 1.0369\n",
      "127/300, train_loss: 4.1490\n",
      "128/300, train_loss: 1.8824\n",
      "129/300, train_loss: 7.5385\n",
      "130/300, train_loss: 2.8633\n",
      "131/300, train_loss: 6.5380\n",
      "132/300, train_loss: 3.4982\n",
      "133/300, train_loss: 5.8208\n",
      "134/300, train_loss: 7.2972\n",
      "135/300, train_loss: 5.7469\n",
      "136/300, train_loss: 1.1698\n",
      "137/300, train_loss: 0.6629\n",
      "138/300, train_loss: 2.7571\n",
      "139/300, train_loss: 6.4364\n",
      "140/300, train_loss: 2.9985\n",
      "141/300, train_loss: 3.4203\n",
      "142/300, train_loss: 3.1496\n",
      "143/300, train_loss: 4.8174\n",
      "144/300, train_loss: 1.0543\n",
      "145/300, train_loss: 7.5776\n",
      "146/300, train_loss: 1.0776\n",
      "147/300, train_loss: 1.3869\n",
      "148/300, train_loss: 6.6402\n",
      "149/300, train_loss: 0.2033\n",
      "150/300, train_loss: 4.8247\n",
      "151/300, train_loss: 5.4025\n",
      "152/300, train_loss: 4.4861\n",
      "153/300, train_loss: 2.9324\n",
      "154/300, train_loss: 0.6104\n",
      "155/300, train_loss: 4.9973\n",
      "156/300, train_loss: 3.7580\n",
      "157/300, train_loss: 2.6292\n",
      "158/300, train_loss: 3.1207\n",
      "159/300, train_loss: 9.1009\n",
      "160/300, train_loss: 3.1376\n",
      "161/300, train_loss: 2.3544\n",
      "162/300, train_loss: 6.4867\n",
      "163/300, train_loss: 2.6754\n",
      "164/300, train_loss: 6.3612\n",
      "165/300, train_loss: 1.0804\n",
      "166/300, train_loss: 5.2336\n",
      "167/300, train_loss: 5.6470\n",
      "168/300, train_loss: 1.6513\n",
      "169/300, train_loss: 2.2236\n",
      "170/300, train_loss: 6.3022\n",
      "171/300, train_loss: 1.5089\n",
      "172/300, train_loss: 1.7092\n",
      "173/300, train_loss: 1.1455\n",
      "174/300, train_loss: 1.6105\n",
      "175/300, train_loss: 2.0861\n",
      "176/300, train_loss: 1.2620\n",
      "177/300, train_loss: 1.4566\n",
      "178/300, train_loss: 3.6142\n",
      "179/300, train_loss: 0.6411\n",
      "180/300, train_loss: 1.7085\n",
      "181/300, train_loss: 1.2099\n",
      "182/300, train_loss: 4.2738\n",
      "183/300, train_loss: 2.6156\n",
      "184/300, train_loss: 7.1964\n",
      "185/300, train_loss: 0.2480\n",
      "186/300, train_loss: 2.6691\n",
      "187/300, train_loss: 9.0547\n",
      "188/300, train_loss: 8.4716\n",
      "189/300, train_loss: 2.3489\n",
      "190/300, train_loss: 2.9319\n",
      "191/300, train_loss: 5.7738\n",
      "192/300, train_loss: 5.3837\n",
      "193/300, train_loss: 7.7047\n",
      "194/300, train_loss: 5.3916\n",
      "195/300, train_loss: 8.9693\n",
      "196/300, train_loss: 3.9832\n",
      "197/300, train_loss: 0.9599\n",
      "198/300, train_loss: 0.4201\n",
      "199/300, train_loss: 3.4722\n",
      "200/300, train_loss: 10.6520\n",
      "201/300, train_loss: 3.7313\n",
      "202/300, train_loss: 2.8014\n",
      "203/300, train_loss: 1.5153\n",
      "204/300, train_loss: 3.5859\n",
      "205/300, train_loss: 6.2925\n",
      "206/300, train_loss: 5.1480\n",
      "207/300, train_loss: 5.7675\n",
      "208/300, train_loss: 0.7939\n",
      "209/300, train_loss: 3.9160\n",
      "210/300, train_loss: 3.8431\n",
      "211/300, train_loss: 8.1729\n",
      "212/300, train_loss: 5.2480\n",
      "213/300, train_loss: 5.5725\n",
      "214/300, train_loss: 1.8697\n",
      "215/300, train_loss: 2.7254\n",
      "216/300, train_loss: 1.4881\n",
      "217/300, train_loss: 6.8948\n",
      "218/300, train_loss: 6.2525\n",
      "219/300, train_loss: 4.4595\n",
      "220/300, train_loss: 1.5998\n",
      "221/300, train_loss: 5.5160\n",
      "222/300, train_loss: 6.0145\n",
      "223/300, train_loss: 3.6852\n",
      "224/300, train_loss: 8.4699\n",
      "225/300, train_loss: 4.0832\n",
      "226/300, train_loss: 1.9479\n",
      "227/300, train_loss: 4.9612\n",
      "228/300, train_loss: 3.0364\n",
      "229/300, train_loss: 8.2347\n",
      "230/300, train_loss: 4.3136\n",
      "231/300, train_loss: 5.5128\n",
      "232/300, train_loss: 7.8931\n",
      "233/300, train_loss: 2.9334\n",
      "234/300, train_loss: 7.6010\n",
      "235/300, train_loss: 2.1331\n",
      "236/300, train_loss: 2.6143\n",
      "237/300, train_loss: 5.8705\n",
      "238/300, train_loss: 0.8097\n",
      "239/300, train_loss: 6.3707\n",
      "240/300, train_loss: 0.2058\n",
      "241/300, train_loss: 6.9023\n",
      "242/300, train_loss: 4.3899\n",
      "243/300, train_loss: 2.7060\n",
      "244/300, train_loss: 4.4356\n",
      "245/300, train_loss: 1.9308\n",
      "246/300, train_loss: 3.7320\n",
      "247/300, train_loss: 4.7193\n",
      "248/300, train_loss: 0.1133\n",
      "249/300, train_loss: 6.3738\n",
      "250/300, train_loss: 2.0696\n",
      "251/300, train_loss: 5.4784\n",
      "252/300, train_loss: 1.3862\n",
      "253/300, train_loss: 3.6507\n",
      "254/300, train_loss: 2.3202\n",
      "255/300, train_loss: 0.1041\n",
      "256/300, train_loss: 3.4884\n",
      "257/300, train_loss: 1.5226\n",
      "258/300, train_loss: 4.1532\n",
      "259/300, train_loss: 5.4405\n",
      "260/300, train_loss: 5.7701\n",
      "261/300, train_loss: 6.3083\n",
      "262/300, train_loss: 6.7733\n",
      "263/300, train_loss: 4.7640\n",
      "264/300, train_loss: 1.9782\n",
      "265/300, train_loss: 2.3408\n",
      "266/300, train_loss: 2.0244\n",
      "267/300, train_loss: 3.4069\n",
      "268/300, train_loss: 0.8123\n",
      "269/300, train_loss: 4.8807\n",
      "270/300, train_loss: 8.2206\n",
      "271/300, train_loss: 8.9148\n",
      "272/300, train_loss: 0.9158\n",
      "273/300, train_loss: 1.3915\n",
      "274/300, train_loss: 6.1893\n",
      "275/300, train_loss: 3.5335\n",
      "276/300, train_loss: 2.5593\n",
      "277/300, train_loss: 2.4014\n",
      "278/300, train_loss: 5.0122\n",
      "279/300, train_loss: 3.5935\n",
      "280/300, train_loss: 2.9762\n",
      "281/300, train_loss: 1.0645\n",
      "282/300, train_loss: 3.4724\n",
      "283/300, train_loss: 2.2689\n",
      "284/300, train_loss: 3.0812\n",
      "285/300, train_loss: 2.0695\n",
      "286/300, train_loss: 3.8001\n",
      "287/300, train_loss: 4.7329\n",
      "288/300, train_loss: 3.0812\n",
      "289/300, train_loss: 0.0814\n",
      "290/300, train_loss: 7.2823\n",
      "291/300, train_loss: 2.6503\n",
      "292/300, train_loss: 7.3249\n",
      "293/300, train_loss: 7.4053\n",
      "294/300, train_loss: 2.0115\n",
      "295/300, train_loss: 0.0346\n",
      "296/300, train_loss: 3.7188\n",
      "297/300, train_loss: 1.7394\n",
      "298/300, train_loss: 7.9737\n",
      "299/300, train_loss: 5.6007\n",
      "300/300, train_loss: 3.7643\n",
      "epoch 2 average loss: 3.8538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved new best metric model = /HippoDense/EfficientNet_04/EfficientNet_04_20220607_012913.pickle\n",
      "Current epoch: 2 current accuracy: 0.0469 \n",
      "Best accuracy: 0.0469 at epoch 2\n",
      "----------\n",
      "epoch 3/150\n",
      "1/300, train_loss: 8.1427\n",
      "2/300, train_loss: 4.9983\n",
      "3/300, train_loss: 3.9716\n",
      "4/300, train_loss: 8.2222\n",
      "5/300, train_loss: 7.7190\n",
      "6/300, train_loss: 9.0340\n",
      "7/300, train_loss: 2.5933\n",
      "8/300, train_loss: 2.4024\n",
      "9/300, train_loss: 8.0016\n",
      "10/300, train_loss: 4.5222\n",
      "11/300, train_loss: 4.0070\n",
      "12/300, train_loss: 2.2484\n",
      "13/300, train_loss: 0.8554\n",
      "14/300, train_loss: 3.0014\n",
      "15/300, train_loss: 0.8351\n",
      "16/300, train_loss: 5.7832\n",
      "17/300, train_loss: 2.1250\n",
      "18/300, train_loss: 3.2358\n",
      "19/300, train_loss: 2.0252\n",
      "20/300, train_loss: 3.1193\n",
      "21/300, train_loss: 6.7036\n",
      "22/300, train_loss: 1.4387\n",
      "23/300, train_loss: 0.4315\n",
      "24/300, train_loss: 5.8094\n",
      "25/300, train_loss: 5.3148\n",
      "26/300, train_loss: 1.8241\n",
      "27/300, train_loss: 0.2530\n",
      "28/300, train_loss: 5.5512\n",
      "29/300, train_loss: 2.7929\n",
      "30/300, train_loss: 1.9221\n",
      "31/300, train_loss: 5.5590\n",
      "32/300, train_loss: 6.6180\n",
      "33/300, train_loss: 5.4813\n",
      "34/300, train_loss: 6.2485\n",
      "35/300, train_loss: 4.9167\n",
      "36/300, train_loss: 1.6901\n",
      "37/300, train_loss: 0.9178\n",
      "38/300, train_loss: 6.2393\n",
      "39/300, train_loss: 0.5638\n",
      "40/300, train_loss: 2.9244\n",
      "41/300, train_loss: 6.1535\n",
      "42/300, train_loss: 8.9119\n",
      "43/300, train_loss: 4.3095\n",
      "44/300, train_loss: 4.8663\n",
      "45/300, train_loss: 2.5439\n",
      "46/300, train_loss: 6.0319\n",
      "47/300, train_loss: 2.1802\n",
      "48/300, train_loss: 5.4719\n",
      "49/300, train_loss: 1.8351\n",
      "50/300, train_loss: 3.8206\n",
      "51/300, train_loss: 7.8241\n",
      "52/300, train_loss: 2.6690\n",
      "53/300, train_loss: 5.9968\n",
      "54/300, train_loss: 3.4776\n",
      "55/300, train_loss: 1.3053\n",
      "56/300, train_loss: 0.2372\n",
      "57/300, train_loss: 2.0266\n",
      "58/300, train_loss: 8.8339\n",
      "59/300, train_loss: 4.6166\n",
      "60/300, train_loss: 6.1654\n",
      "61/300, train_loss: 9.6235\n",
      "62/300, train_loss: 3.3249\n",
      "63/300, train_loss: 4.9015\n",
      "64/300, train_loss: 0.1271\n",
      "65/300, train_loss: 4.0270\n",
      "66/300, train_loss: 0.8831\n",
      "67/300, train_loss: 1.9618\n",
      "68/300, train_loss: 6.4490\n",
      "69/300, train_loss: 0.8112\n",
      "70/300, train_loss: 1.2483\n",
      "71/300, train_loss: 3.0151\n",
      "72/300, train_loss: 4.9896\n",
      "73/300, train_loss: 5.8915\n",
      "74/300, train_loss: 1.4953\n",
      "75/300, train_loss: 0.0727\n",
      "76/300, train_loss: 2.3274\n",
      "77/300, train_loss: 2.1628\n",
      "78/300, train_loss: 1.7142\n",
      "79/300, train_loss: 0.0993\n",
      "80/300, train_loss: 8.5742\n",
      "81/300, train_loss: 2.1212\n",
      "82/300, train_loss: 1.8494\n",
      "83/300, train_loss: 4.1576\n",
      "84/300, train_loss: 1.1185\n",
      "85/300, train_loss: 0.1258\n",
      "86/300, train_loss: 5.8020\n",
      "87/300, train_loss: 1.7117\n",
      "88/300, train_loss: 3.2437\n",
      "89/300, train_loss: 3.8607\n",
      "90/300, train_loss: 6.2107\n",
      "91/300, train_loss: 6.7501\n",
      "92/300, train_loss: 1.0405\n",
      "93/300, train_loss: 4.6770\n",
      "94/300, train_loss: 2.3159\n",
      "95/300, train_loss: 3.0375\n",
      "96/300, train_loss: 3.8181\n",
      "97/300, train_loss: 1.6486\n",
      "98/300, train_loss: 5.5604\n",
      "99/300, train_loss: 1.7326\n",
      "100/300, train_loss: 5.4420\n",
      "101/300, train_loss: 3.1997\n",
      "102/300, train_loss: 1.6225\n",
      "103/300, train_loss: 2.4929\n",
      "104/300, train_loss: 3.0859\n",
      "105/300, train_loss: 10.5729\n",
      "106/300, train_loss: 3.9142\n",
      "107/300, train_loss: 0.3295\n",
      "108/300, train_loss: 2.2065\n",
      "109/300, train_loss: 1.9037\n",
      "110/300, train_loss: 6.2681\n",
      "111/300, train_loss: 3.6990\n",
      "112/300, train_loss: 4.1606\n",
      "113/300, train_loss: 6.4584\n",
      "114/300, train_loss: 2.6985\n",
      "115/300, train_loss: 4.9606\n",
      "116/300, train_loss: 3.4873\n",
      "117/300, train_loss: 8.5591\n",
      "118/300, train_loss: 0.7516\n",
      "119/300, train_loss: 7.5225\n",
      "120/300, train_loss: 0.3960\n",
      "121/300, train_loss: 4.5507\n",
      "122/300, train_loss: 3.4253\n",
      "123/300, train_loss: 6.3409\n",
      "124/300, train_loss: 0.8584\n",
      "125/300, train_loss: 4.4705\n",
      "126/300, train_loss: 1.5990\n",
      "127/300, train_loss: 2.8437\n",
      "128/300, train_loss: 2.6393\n",
      "129/300, train_loss: 2.5023\n",
      "130/300, train_loss: 3.1670\n",
      "131/300, train_loss: 4.5576\n",
      "132/300, train_loss: 3.6161\n",
      "133/300, train_loss: 4.9110\n",
      "134/300, train_loss: 4.8725\n",
      "135/300, train_loss: 2.9607\n",
      "136/300, train_loss: 2.0765\n",
      "137/300, train_loss: 1.7105\n",
      "138/300, train_loss: 5.1838\n",
      "139/300, train_loss: 3.1576\n",
      "140/300, train_loss: 10.8949\n",
      "141/300, train_loss: 5.8809\n",
      "142/300, train_loss: 3.6792\n",
      "143/300, train_loss: 0.2778\n",
      "144/300, train_loss: 2.7376\n",
      "145/300, train_loss: 1.7520\n",
      "146/300, train_loss: 3.9162\n",
      "147/300, train_loss: 0.5784\n",
      "148/300, train_loss: 2.4289\n",
      "149/300, train_loss: 2.4216\n",
      "150/300, train_loss: 5.1437\n",
      "151/300, train_loss: 2.2446\n",
      "152/300, train_loss: 6.6485\n",
      "153/300, train_loss: 4.7126\n",
      "154/300, train_loss: 1.1232\n",
      "155/300, train_loss: 1.3836\n",
      "156/300, train_loss: 2.1503\n",
      "157/300, train_loss: 5.8693\n",
      "158/300, train_loss: 0.6324\n",
      "159/300, train_loss: 7.1735\n",
      "160/300, train_loss: 3.7085\n",
      "161/300, train_loss: 3.3409\n",
      "162/300, train_loss: 4.6971\n",
      "163/300, train_loss: 7.4540\n",
      "164/300, train_loss: 4.5622\n",
      "165/300, train_loss: 2.5309\n",
      "166/300, train_loss: 2.3490\n",
      "167/300, train_loss: 0.3521\n",
      "168/300, train_loss: 0.6753\n",
      "169/300, train_loss: 9.5775\n",
      "170/300, train_loss: 7.1848\n",
      "171/300, train_loss: 6.0660\n",
      "172/300, train_loss: 6.7883\n",
      "173/300, train_loss: 3.3797\n",
      "174/300, train_loss: 2.1307\n",
      "175/300, train_loss: 0.7799\n",
      "176/300, train_loss: 3.0514\n",
      "177/300, train_loss: 4.0779\n",
      "178/300, train_loss: 2.9536\n",
      "179/300, train_loss: 9.2526\n",
      "180/300, train_loss: 2.4837\n",
      "181/300, train_loss: 3.2194\n",
      "182/300, train_loss: 1.3890\n",
      "183/300, train_loss: 4.0737\n",
      "184/300, train_loss: 2.3210\n",
      "185/300, train_loss: 4.6434\n",
      "186/300, train_loss: 6.1307\n",
      "187/300, train_loss: 0.6957\n",
      "188/300, train_loss: 5.3052\n",
      "189/300, train_loss: 4.6657\n",
      "190/300, train_loss: 7.1698\n",
      "191/300, train_loss: 3.7549\n",
      "192/300, train_loss: 0.1796\n",
      "193/300, train_loss: 4.3124\n",
      "194/300, train_loss: 4.3086\n",
      "195/300, train_loss: 5.2121\n",
      "196/300, train_loss: 7.6278\n",
      "197/300, train_loss: 3.9675\n",
      "198/300, train_loss: 1.5426\n",
      "199/300, train_loss: 8.3240\n",
      "200/300, train_loss: 1.1495\n",
      "201/300, train_loss: 8.4435\n",
      "202/300, train_loss: 0.7094\n",
      "203/300, train_loss: 6.6623\n",
      "204/300, train_loss: 3.2436\n",
      "205/300, train_loss: 5.7917\n",
      "206/300, train_loss: 2.3660\n",
      "207/300, train_loss: 5.8745\n",
      "208/300, train_loss: 6.1894\n",
      "209/300, train_loss: 1.2644\n",
      "210/300, train_loss: 0.8146\n",
      "211/300, train_loss: 4.6816\n",
      "212/300, train_loss: 6.5940\n",
      "213/300, train_loss: 3.9233\n",
      "214/300, train_loss: 4.2063\n",
      "215/300, train_loss: 3.2935\n",
      "216/300, train_loss: 1.0823\n",
      "217/300, train_loss: 1.9464\n",
      "218/300, train_loss: 2.7028\n",
      "219/300, train_loss: 2.7910\n",
      "220/300, train_loss: 1.4166\n",
      "221/300, train_loss: 7.7781\n",
      "222/300, train_loss: 0.0520\n",
      "223/300, train_loss: 2.0744\n",
      "224/300, train_loss: 7.6471\n",
      "225/300, train_loss: 3.8221\n",
      "226/300, train_loss: 2.7027\n",
      "227/300, train_loss: 8.1963\n",
      "228/300, train_loss: 1.8492\n",
      "229/300, train_loss: 1.5058\n",
      "230/300, train_loss: 2.2950\n",
      "231/300, train_loss: 6.5213\n",
      "232/300, train_loss: 2.9716\n",
      "233/300, train_loss: 6.5001\n",
      "234/300, train_loss: 3.8859\n",
      "235/300, train_loss: 6.5427\n",
      "236/300, train_loss: 8.5558\n",
      "237/300, train_loss: 3.5822\n",
      "238/300, train_loss: 5.9380\n",
      "239/300, train_loss: 7.3679\n",
      "240/300, train_loss: 5.2030\n",
      "241/300, train_loss: 1.8634\n",
      "242/300, train_loss: 8.2378\n",
      "243/300, train_loss: 4.2971\n",
      "244/300, train_loss: 6.1204\n",
      "245/300, train_loss: 6.7446\n",
      "246/300, train_loss: 4.8994\n",
      "247/300, train_loss: 3.7438\n",
      "248/300, train_loss: 1.9534\n",
      "249/300, train_loss: 0.2709\n",
      "250/300, train_loss: 1.9271\n",
      "251/300, train_loss: 2.3690\n",
      "252/300, train_loss: 1.0079\n",
      "253/300, train_loss: 3.1302\n",
      "254/300, train_loss: 2.1407\n",
      "255/300, train_loss: 6.1405\n",
      "256/300, train_loss: 3.6854\n",
      "257/300, train_loss: 0.1532\n",
      "258/300, train_loss: 1.7240\n",
      "259/300, train_loss: 4.8999\n",
      "260/300, train_loss: 3.2022\n",
      "261/300, train_loss: 8.6850\n",
      "262/300, train_loss: 4.5188\n",
      "263/300, train_loss: 4.8869\n",
      "264/300, train_loss: 6.4799\n",
      "265/300, train_loss: 4.8680\n",
      "266/300, train_loss: 6.2120\n",
      "267/300, train_loss: 5.3866\n",
      "268/300, train_loss: 0.8917\n",
      "269/300, train_loss: 3.6035\n",
      "270/300, train_loss: 6.8589\n",
      "271/300, train_loss: 2.0653\n",
      "272/300, train_loss: 4.4161\n",
      "273/300, train_loss: 4.2254\n",
      "274/300, train_loss: 6.1317\n",
      "275/300, train_loss: 5.4141\n",
      "276/300, train_loss: 0.7086\n",
      "277/300, train_loss: 2.6375\n",
      "278/300, train_loss: 1.6939\n",
      "279/300, train_loss: 5.2610\n",
      "280/300, train_loss: 0.4541\n",
      "281/300, train_loss: 1.2708\n",
      "282/300, train_loss: 6.1876\n",
      "283/300, train_loss: 4.1289\n",
      "284/300, train_loss: 3.9496\n",
      "285/300, train_loss: 3.5293\n",
      "286/300, train_loss: 4.9125\n",
      "287/300, train_loss: 1.8171\n",
      "288/300, train_loss: 2.7850\n",
      "289/300, train_loss: 2.1352\n",
      "290/300, train_loss: 3.0146\n",
      "291/300, train_loss: 4.6943\n",
      "292/300, train_loss: 6.9903\n",
      "293/300, train_loss: 6.5329\n",
      "294/300, train_loss: 5.3004\n",
      "295/300, train_loss: 2.6992\n",
      "296/300, train_loss: 1.8411\n",
      "297/300, train_loss: 8.6991\n",
      "298/300, train_loss: 1.7229\n",
      "299/300, train_loss: 4.2744\n",
      "300/300, train_loss: 2.4903\n",
      "epoch 3 average loss: 3.8796\n",
      "----------\n",
      "epoch 4/150\n",
      "1/300, train_loss: 1.4099\n",
      "2/300, train_loss: 4.9212\n",
      "3/300, train_loss: 0.3288\n",
      "4/300, train_loss: 1.7726\n",
      "5/300, train_loss: 1.2483\n",
      "6/300, train_loss: 5.2063\n",
      "7/300, train_loss: 6.4717\n",
      "8/300, train_loss: 6.2784\n",
      "9/300, train_loss: 1.7257\n",
      "10/300, train_loss: 5.0494\n",
      "11/300, train_loss: 1.9012\n",
      "12/300, train_loss: 6.6498\n",
      "13/300, train_loss: 2.5204\n",
      "14/300, train_loss: 0.2845\n",
      "15/300, train_loss: 1.3491\n",
      "16/300, train_loss: 6.5016\n",
      "17/300, train_loss: 6.2918\n",
      "18/300, train_loss: 2.8791\n",
      "19/300, train_loss: 3.9600\n",
      "20/300, train_loss: 1.9911\n",
      "21/300, train_loss: 3.5994\n",
      "22/300, train_loss: 5.7272\n",
      "23/300, train_loss: 1.7020\n",
      "24/300, train_loss: 5.9091\n",
      "25/300, train_loss: 5.5662\n",
      "26/300, train_loss: 5.0029\n",
      "27/300, train_loss: 3.6029\n",
      "28/300, train_loss: 2.7928\n",
      "29/300, train_loss: 2.9734\n",
      "30/300, train_loss: 8.5838\n",
      "31/300, train_loss: 5.2011\n",
      "32/300, train_loss: 6.0878\n",
      "33/300, train_loss: 1.5125\n",
      "34/300, train_loss: 0.0596\n",
      "35/300, train_loss: 0.8237\n",
      "36/300, train_loss: 3.8072\n",
      "37/300, train_loss: 5.4531\n",
      "38/300, train_loss: 7.5659\n",
      "39/300, train_loss: 5.0591\n",
      "40/300, train_loss: 2.5945\n",
      "41/300, train_loss: 5.2675\n",
      "42/300, train_loss: 2.2971\n",
      "43/300, train_loss: 2.8978\n",
      "44/300, train_loss: 9.2734\n",
      "45/300, train_loss: 4.2831\n",
      "46/300, train_loss: 4.2294\n",
      "47/300, train_loss: 0.4779\n",
      "48/300, train_loss: 3.6170\n",
      "49/300, train_loss: 1.8526\n",
      "50/300, train_loss: 0.0061\n",
      "51/300, train_loss: 6.7204\n",
      "52/300, train_loss: 1.5829\n",
      "53/300, train_loss: 7.1813\n",
      "54/300, train_loss: 0.0485\n",
      "55/300, train_loss: 3.6273\n",
      "56/300, train_loss: 6.5404\n",
      "57/300, train_loss: 4.2970\n",
      "58/300, train_loss: 2.1048\n",
      "59/300, train_loss: 3.1679\n",
      "60/300, train_loss: 3.1344\n",
      "61/300, train_loss: 1.9988\n",
      "62/300, train_loss: 5.2489\n",
      "63/300, train_loss: 1.7857\n",
      "64/300, train_loss: 3.1664\n",
      "65/300, train_loss: 3.6615\n",
      "66/300, train_loss: 6.9051\n",
      "67/300, train_loss: 6.3456\n",
      "68/300, train_loss: 7.0019\n",
      "69/300, train_loss: 3.3468\n",
      "70/300, train_loss: 3.2666\n",
      "71/300, train_loss: 4.8510\n",
      "72/300, train_loss: 1.1753\n",
      "73/300, train_loss: 1.5038\n",
      "74/300, train_loss: 1.6780\n",
      "75/300, train_loss: 6.5763\n",
      "76/300, train_loss: 3.8836\n",
      "77/300, train_loss: 0.4136\n",
      "78/300, train_loss: 8.7965\n",
      "79/300, train_loss: 3.6710\n",
      "80/300, train_loss: 5.7943\n",
      "81/300, train_loss: 4.5952\n",
      "82/300, train_loss: 5.9480\n",
      "83/300, train_loss: 6.6559\n",
      "84/300, train_loss: 2.2292\n",
      "85/300, train_loss: 1.3061\n",
      "86/300, train_loss: 4.5829\n",
      "87/300, train_loss: 3.4155\n",
      "88/300, train_loss: 3.6619\n",
      "89/300, train_loss: 6.3063\n",
      "90/300, train_loss: 1.4404\n",
      "91/300, train_loss: 2.7093\n",
      "92/300, train_loss: 4.9766\n",
      "93/300, train_loss: 1.3175\n",
      "94/300, train_loss: 4.4800\n",
      "95/300, train_loss: 5.5785\n",
      "96/300, train_loss: 2.7888\n",
      "97/300, train_loss: 7.7441\n",
      "98/300, train_loss: 5.3647\n",
      "99/300, train_loss: 3.9977\n",
      "100/300, train_loss: 2.4622\n",
      "101/300, train_loss: 4.6687\n",
      "102/300, train_loss: 4.7621\n",
      "103/300, train_loss: 2.0295\n",
      "104/300, train_loss: 2.6010\n",
      "105/300, train_loss: 2.8072\n",
      "106/300, train_loss: 2.7255\n",
      "107/300, train_loss: 1.9946\n",
      "108/300, train_loss: 1.8437\n",
      "109/300, train_loss: 1.9525\n",
      "110/300, train_loss: 4.3200\n",
      "111/300, train_loss: 3.8666\n",
      "112/300, train_loss: 0.8410\n",
      "113/300, train_loss: 5.4317\n",
      "114/300, train_loss: 7.2623\n",
      "115/300, train_loss: 7.8496\n",
      "116/300, train_loss: 1.8937\n",
      "117/300, train_loss: 6.0526\n",
      "118/300, train_loss: 2.4337\n",
      "119/300, train_loss: 0.8176\n",
      "120/300, train_loss: 10.3608\n",
      "121/300, train_loss: 3.4934\n",
      "122/300, train_loss: 3.5789\n",
      "123/300, train_loss: 3.1063\n",
      "124/300, train_loss: 0.2508\n",
      "125/300, train_loss: 4.6673\n",
      "126/300, train_loss: 1.9102\n",
      "127/300, train_loss: 5.3817\n",
      "128/300, train_loss: 6.2721\n",
      "129/300, train_loss: 4.1473\n",
      "130/300, train_loss: 8.5873\n",
      "131/300, train_loss: 2.1758\n",
      "132/300, train_loss: 1.0539\n",
      "133/300, train_loss: 0.0851\n",
      "134/300, train_loss: 8.4762\n",
      "135/300, train_loss: 1.3896\n",
      "136/300, train_loss: 6.9268\n",
      "137/300, train_loss: 0.9961\n",
      "138/300, train_loss: 4.9780\n",
      "139/300, train_loss: 7.3220\n",
      "140/300, train_loss: 1.4782\n",
      "141/300, train_loss: 0.6374\n",
      "142/300, train_loss: 5.5978\n",
      "143/300, train_loss: 1.8545\n",
      "144/300, train_loss: 0.1822\n",
      "145/300, train_loss: 7.0367\n",
      "146/300, train_loss: 1.0528\n",
      "147/300, train_loss: 0.9838\n",
      "148/300, train_loss: 1.0912\n",
      "149/300, train_loss: 2.4160\n",
      "150/300, train_loss: 3.0103\n",
      "151/300, train_loss: 1.1119\n",
      "152/300, train_loss: 5.7916\n",
      "153/300, train_loss: 8.7449\n",
      "154/300, train_loss: 0.5244\n",
      "155/300, train_loss: 1.0674\n",
      "156/300, train_loss: 5.8395\n",
      "157/300, train_loss: 2.6453\n",
      "158/300, train_loss: 2.5722\n",
      "159/300, train_loss: 5.5924\n",
      "160/300, train_loss: 2.1253\n",
      "161/300, train_loss: 6.2440\n",
      "162/300, train_loss: 6.5197\n",
      "163/300, train_loss: 4.5027\n",
      "164/300, train_loss: 2.5483\n",
      "165/300, train_loss: 3.5502\n",
      "166/300, train_loss: 4.7518\n",
      "167/300, train_loss: 4.1937\n",
      "168/300, train_loss: 0.6803\n",
      "169/300, train_loss: 8.7325\n",
      "170/300, train_loss: 1.4542\n",
      "171/300, train_loss: 6.5967\n",
      "172/300, train_loss: 4.4522\n",
      "173/300, train_loss: 1.3337\n",
      "174/300, train_loss: 1.2402\n",
      "175/300, train_loss: 3.5099\n",
      "176/300, train_loss: 4.0223\n",
      "177/300, train_loss: 0.8637\n",
      "178/300, train_loss: 1.6622\n",
      "179/300, train_loss: 3.3149\n",
      "180/300, train_loss: 1.5493\n",
      "181/300, train_loss: 6.5944\n",
      "182/300, train_loss: 6.3667\n",
      "183/300, train_loss: 5.1925\n",
      "184/300, train_loss: 0.9024\n",
      "185/300, train_loss: 7.0482\n",
      "186/300, train_loss: 5.7864\n",
      "187/300, train_loss: 3.8851\n",
      "188/300, train_loss: 5.8686\n",
      "189/300, train_loss: 2.3373\n",
      "190/300, train_loss: 1.2141\n",
      "191/300, train_loss: 8.6377\n",
      "192/300, train_loss: 2.1934\n",
      "193/300, train_loss: 3.4050\n",
      "194/300, train_loss: 3.1151\n",
      "195/300, train_loss: 3.8320\n",
      "196/300, train_loss: 4.9804\n",
      "197/300, train_loss: 5.5161\n",
      "198/300, train_loss: 4.9451\n",
      "199/300, train_loss: 3.0760\n",
      "200/300, train_loss: 8.2543\n",
      "201/300, train_loss: 5.4969\n",
      "202/300, train_loss: 7.0619\n",
      "203/300, train_loss: 3.4427\n",
      "204/300, train_loss: 5.0020\n",
      "205/300, train_loss: 3.3061\n",
      "206/300, train_loss: 7.7164\n",
      "207/300, train_loss: 0.7608\n",
      "208/300, train_loss: 3.4178\n",
      "209/300, train_loss: 1.0485\n",
      "210/300, train_loss: 7.7498\n",
      "211/300, train_loss: 7.8741\n",
      "212/300, train_loss: 6.3606\n",
      "213/300, train_loss: 4.0548\n",
      "214/300, train_loss: 3.2141\n",
      "215/300, train_loss: 6.2749\n",
      "216/300, train_loss: 6.6226\n",
      "217/300, train_loss: 1.7068\n",
      "218/300, train_loss: 4.7172\n",
      "219/300, train_loss: 4.8367\n",
      "220/300, train_loss: 9.7123\n",
      "221/300, train_loss: 3.3848\n",
      "222/300, train_loss: 1.7232\n",
      "223/300, train_loss: 0.3777\n",
      "224/300, train_loss: 0.0207\n",
      "225/300, train_loss: 4.5520\n",
      "226/300, train_loss: 2.0336\n",
      "227/300, train_loss: 3.3446\n",
      "228/300, train_loss: 0.7099\n",
      "229/300, train_loss: 0.6960\n",
      "230/300, train_loss: 4.4697\n",
      "231/300, train_loss: 3.4455\n",
      "232/300, train_loss: 1.9074\n",
      "233/300, train_loss: 4.8490\n",
      "234/300, train_loss: 2.3793\n",
      "235/300, train_loss: 1.4455\n",
      "236/300, train_loss: 2.9140\n",
      "237/300, train_loss: 6.4795\n",
      "238/300, train_loss: 7.1324\n",
      "239/300, train_loss: 4.3403\n",
      "240/300, train_loss: 6.6840\n",
      "241/300, train_loss: 4.6488\n",
      "242/300, train_loss: 1.4655\n",
      "243/300, train_loss: 1.0292\n",
      "244/300, train_loss: 3.8831\n",
      "245/300, train_loss: 4.1606\n",
      "246/300, train_loss: 6.8262\n",
      "247/300, train_loss: 8.0356\n",
      "248/300, train_loss: 4.4822\n",
      "249/300, train_loss: 1.5384\n",
      "250/300, train_loss: 1.2242\n",
      "251/300, train_loss: 3.1070\n",
      "252/300, train_loss: 6.9389\n",
      "253/300, train_loss: 3.1979\n",
      "254/300, train_loss: 4.8575\n",
      "255/300, train_loss: 7.9239\n",
      "256/300, train_loss: 1.9794\n",
      "257/300, train_loss: 7.1022\n",
      "258/300, train_loss: 1.3222\n",
      "259/300, train_loss: 5.1664\n",
      "260/300, train_loss: 1.4450\n",
      "261/300, train_loss: 2.9762\n",
      "262/300, train_loss: 2.4794\n",
      "263/300, train_loss: 1.4632\n",
      "264/300, train_loss: 1.5909\n",
      "265/300, train_loss: 10.1316\n",
      "266/300, train_loss: 7.0207\n",
      "267/300, train_loss: 4.8154\n",
      "268/300, train_loss: 6.7286\n",
      "269/300, train_loss: 5.8726\n",
      "270/300, train_loss: 4.5319\n",
      "271/300, train_loss: 0.0694\n",
      "272/300, train_loss: 3.6566\n",
      "273/300, train_loss: 3.6594\n",
      "274/300, train_loss: 5.4186\n",
      "275/300, train_loss: 5.8282\n",
      "276/300, train_loss: 5.8452\n",
      "277/300, train_loss: 6.9145\n",
      "278/300, train_loss: 4.2661\n",
      "279/300, train_loss: 3.4347\n",
      "280/300, train_loss: 4.3616\n",
      "281/300, train_loss: 7.4433\n",
      "282/300, train_loss: 3.9491\n",
      "283/300, train_loss: 3.7165\n",
      "284/300, train_loss: 1.8095\n",
      "285/300, train_loss: 1.4645\n",
      "286/300, train_loss: 3.9214\n",
      "287/300, train_loss: 2.8289\n",
      "288/300, train_loss: 1.3110\n",
      "289/300, train_loss: 3.4007\n",
      "290/300, train_loss: 0.5387\n",
      "291/300, train_loss: 9.1547\n",
      "292/300, train_loss: 6.8994\n",
      "293/300, train_loss: 6.1921\n",
      "294/300, train_loss: 0.9694\n",
      "295/300, train_loss: 1.5842\n",
      "296/300, train_loss: 3.2485\n",
      "297/300, train_loss: 2.6561\n",
      "298/300, train_loss: 7.9493\n",
      "299/300, train_loss: 3.0572\n",
      "300/300, train_loss: 1.4752\n",
      "epoch 4 average loss: 3.9113\n",
      "Current epoch: 4 current accuracy: 0.0469 \n",
      "Best accuracy: 0.0469 at epoch 2\n",
      "----------\n",
      "epoch 5/150\n",
      "1/300, train_loss: 1.8981\n",
      "2/300, train_loss: 5.8703\n",
      "3/300, train_loss: 0.8849\n",
      "4/300, train_loss: 3.7979\n",
      "5/300, train_loss: 2.8652\n",
      "6/300, train_loss: 3.0585\n",
      "7/300, train_loss: 4.0064\n",
      "8/300, train_loss: 8.7144\n",
      "9/300, train_loss: 0.4539\n",
      "10/300, train_loss: 3.9830\n",
      "11/300, train_loss: 6.7164\n",
      "12/300, train_loss: 2.7514\n",
      "13/300, train_loss: 3.8925\n",
      "14/300, train_loss: 2.3671\n",
      "15/300, train_loss: 10.1179\n",
      "16/300, train_loss: 4.6867\n",
      "17/300, train_loss: 2.1109\n",
      "18/300, train_loss: 8.5251\n",
      "19/300, train_loss: 4.7971\n",
      "20/300, train_loss: 7.8623\n",
      "21/300, train_loss: 0.1031\n",
      "22/300, train_loss: 0.9534\n",
      "23/300, train_loss: 8.7263\n",
      "24/300, train_loss: 7.1602\n",
      "25/300, train_loss: 5.3416\n",
      "26/300, train_loss: 2.1530\n",
      "27/300, train_loss: 8.0132\n",
      "28/300, train_loss: 5.0436\n",
      "29/300, train_loss: 1.4560\n",
      "30/300, train_loss: 0.9657\n",
      "31/300, train_loss: 4.2808\n",
      "32/300, train_loss: 2.3248\n",
      "33/300, train_loss: 8.0738\n",
      "34/300, train_loss: 2.6951\n",
      "35/300, train_loss: 5.5006\n",
      "36/300, train_loss: 1.1682\n",
      "37/300, train_loss: 2.6434\n",
      "38/300, train_loss: 2.8348\n",
      "39/300, train_loss: 2.8297\n",
      "40/300, train_loss: 2.8468\n",
      "41/300, train_loss: 4.3266\n",
      "42/300, train_loss: 7.3881\n",
      "43/300, train_loss: 3.2908\n",
      "44/300, train_loss: 5.8953\n",
      "45/300, train_loss: 4.4119\n",
      "46/300, train_loss: 2.4653\n",
      "47/300, train_loss: 1.4814\n",
      "48/300, train_loss: 3.1590\n",
      "49/300, train_loss: 1.3176\n",
      "50/300, train_loss: 8.0883\n",
      "51/300, train_loss: 0.4626\n",
      "52/300, train_loss: 8.8687\n",
      "53/300, train_loss: 4.3919\n",
      "54/300, train_loss: 6.4669\n",
      "55/300, train_loss: 7.6405\n",
      "56/300, train_loss: 3.4293\n",
      "57/300, train_loss: 1.6129\n",
      "58/300, train_loss: 2.6271\n",
      "59/300, train_loss: 0.4310\n",
      "60/300, train_loss: 4.5782\n",
      "61/300, train_loss: 1.8225\n",
      "62/300, train_loss: 1.2939\n",
      "63/300, train_loss: 0.6232\n",
      "64/300, train_loss: 3.8511\n",
      "65/300, train_loss: 6.2176\n",
      "66/300, train_loss: 2.7897\n",
      "67/300, train_loss: 4.9261\n",
      "68/300, train_loss: 2.1097\n",
      "69/300, train_loss: 3.7284\n",
      "70/300, train_loss: 4.1778\n",
      "71/300, train_loss: 1.1302\n",
      "72/300, train_loss: 3.8184\n",
      "73/300, train_loss: 2.0947\n",
      "74/300, train_loss: 3.2419\n",
      "75/300, train_loss: 3.9157\n",
      "76/300, train_loss: 2.1114\n",
      "77/300, train_loss: 1.7570\n",
      "78/300, train_loss: 3.2121\n",
      "79/300, train_loss: 1.5196\n",
      "80/300, train_loss: 3.2420\n",
      "81/300, train_loss: 10.1818\n",
      "82/300, train_loss: 4.5883\n",
      "83/300, train_loss: 0.4120\n",
      "84/300, train_loss: 1.4967\n",
      "85/300, train_loss: 3.4012\n",
      "86/300, train_loss: 0.4844\n",
      "87/300, train_loss: 4.7264\n",
      "88/300, train_loss: 0.4929\n",
      "89/300, train_loss: 7.5103\n",
      "90/300, train_loss: 6.4493\n",
      "91/300, train_loss: 4.1990\n",
      "92/300, train_loss: 1.2634\n",
      "93/300, train_loss: 6.5366\n",
      "94/300, train_loss: 6.6221\n",
      "95/300, train_loss: 4.2450\n",
      "96/300, train_loss: 2.9821\n",
      "97/300, train_loss: 5.4133\n",
      "98/300, train_loss: 3.3819\n",
      "99/300, train_loss: 5.1598\n",
      "100/300, train_loss: 7.9327\n",
      "101/300, train_loss: 3.6555\n",
      "102/300, train_loss: 8.9767\n",
      "103/300, train_loss: 9.1384\n",
      "104/300, train_loss: 7.7365\n",
      "105/300, train_loss: 5.8692\n",
      "106/300, train_loss: 2.3662\n",
      "107/300, train_loss: 3.9943\n",
      "108/300, train_loss: 4.2684\n",
      "109/300, train_loss: 2.3763\n",
      "110/300, train_loss: 7.4629\n",
      "111/300, train_loss: 1.3934\n",
      "112/300, train_loss: 1.6382\n",
      "113/300, train_loss: 7.6870\n",
      "114/300, train_loss: 0.4171\n",
      "115/300, train_loss: 1.1044\n",
      "116/300, train_loss: 4.9063\n",
      "117/300, train_loss: 10.1842\n",
      "118/300, train_loss: 1.9969\n",
      "119/300, train_loss: 3.4425\n",
      "120/300, train_loss: 4.5930\n",
      "121/300, train_loss: 6.7777\n",
      "122/300, train_loss: 8.4776\n",
      "123/300, train_loss: 4.7081\n",
      "124/300, train_loss: 0.3649\n",
      "125/300, train_loss: 7.9931\n",
      "126/300, train_loss: 1.3682\n",
      "127/300, train_loss: 6.5132\n",
      "128/300, train_loss: 4.3856\n",
      "129/300, train_loss: 8.6757\n",
      "130/300, train_loss: 7.3102\n",
      "131/300, train_loss: 6.7072\n",
      "132/300, train_loss: 4.1807\n",
      "133/300, train_loss: 2.3105\n",
      "134/300, train_loss: 1.3698\n",
      "135/300, train_loss: 2.5137\n",
      "136/300, train_loss: 0.1151\n",
      "137/300, train_loss: 0.5859\n",
      "138/300, train_loss: 3.3622\n",
      "139/300, train_loss: 1.9278\n",
      "140/300, train_loss: 4.3032\n",
      "141/300, train_loss: 5.4231\n",
      "142/300, train_loss: 4.4008\n",
      "143/300, train_loss: 6.1427\n",
      "144/300, train_loss: 7.9404\n",
      "145/300, train_loss: 7.9963\n",
      "146/300, train_loss: 3.6405\n",
      "147/300, train_loss: 2.6858\n",
      "148/300, train_loss: 7.8678\n",
      "149/300, train_loss: 0.2689\n",
      "150/300, train_loss: 8.2134\n",
      "151/300, train_loss: 5.2495\n",
      "152/300, train_loss: 4.8134\n",
      "153/300, train_loss: 7.3125\n",
      "154/300, train_loss: 5.6349\n",
      "155/300, train_loss: 0.2920\n",
      "156/300, train_loss: 5.2387\n",
      "157/300, train_loss: 4.5808\n",
      "158/300, train_loss: 6.8504\n",
      "159/300, train_loss: 3.6891\n",
      "160/300, train_loss: 2.1979\n",
      "161/300, train_loss: 5.1403\n",
      "162/300, train_loss: 0.2679\n",
      "163/300, train_loss: 5.8001\n",
      "164/300, train_loss: 1.8086\n",
      "165/300, train_loss: 0.1336\n",
      "166/300, train_loss: 3.1255\n",
      "167/300, train_loss: 2.3947\n",
      "168/300, train_loss: 5.2272\n",
      "169/300, train_loss: 8.6743\n",
      "170/300, train_loss: 5.9910\n",
      "171/300, train_loss: 1.0782\n",
      "172/300, train_loss: 11.5671\n",
      "173/300, train_loss: 3.0638\n",
      "174/300, train_loss: 2.1945\n",
      "175/300, train_loss: 6.4201\n",
      "176/300, train_loss: 6.8593\n",
      "177/300, train_loss: 1.7559\n",
      "178/300, train_loss: 1.9316\n",
      "179/300, train_loss: 1.8418\n",
      "180/300, train_loss: 4.5858\n",
      "181/300, train_loss: 3.6085\n",
      "182/300, train_loss: 1.1275\n",
      "183/300, train_loss: 1.9591\n",
      "184/300, train_loss: 0.8853\n",
      "185/300, train_loss: 3.7309\n",
      "186/300, train_loss: 2.7389\n",
      "187/300, train_loss: 4.3303\n",
      "188/300, train_loss: 0.6625\n",
      "189/300, train_loss: 3.5197\n",
      "190/300, train_loss: 3.7285\n",
      "191/300, train_loss: 4.2004\n",
      "192/300, train_loss: 4.5606\n",
      "193/300, train_loss: 2.5406\n",
      "194/300, train_loss: 1.7033\n",
      "195/300, train_loss: 3.9534\n",
      "196/300, train_loss: 0.0699\n",
      "197/300, train_loss: 4.3464\n",
      "198/300, train_loss: 5.3376\n",
      "199/300, train_loss: 1.2585\n",
      "200/300, train_loss: 2.6926\n",
      "201/300, train_loss: 1.2038\n",
      "202/300, train_loss: 2.5904\n",
      "203/300, train_loss: 1.4190\n",
      "204/300, train_loss: 2.9716\n",
      "205/300, train_loss: 1.0873\n",
      "206/300, train_loss: 7.4615\n",
      "207/300, train_loss: 2.7958\n",
      "208/300, train_loss: 2.0088\n",
      "209/300, train_loss: 4.9686\n",
      "210/300, train_loss: 5.0905\n",
      "211/300, train_loss: 5.1072\n",
      "212/300, train_loss: 8.9282\n",
      "213/300, train_loss: 9.2280\n",
      "214/300, train_loss: 2.8406\n",
      "215/300, train_loss: 2.0250\n",
      "216/300, train_loss: 3.4983\n",
      "217/300, train_loss: 6.5789\n",
      "218/300, train_loss: 2.8560\n",
      "219/300, train_loss: 1.9426\n",
      "220/300, train_loss: 5.2354\n",
      "221/300, train_loss: 6.5654\n",
      "222/300, train_loss: 4.4325\n",
      "223/300, train_loss: 4.0397\n",
      "224/300, train_loss: 2.2059\n",
      "225/300, train_loss: 3.0627\n",
      "226/300, train_loss: 1.0746\n",
      "227/300, train_loss: 5.1728\n",
      "228/300, train_loss: 1.7779\n",
      "229/300, train_loss: 5.8943\n",
      "230/300, train_loss: 8.2729\n",
      "231/300, train_loss: 2.9866\n",
      "232/300, train_loss: 6.4222\n",
      "233/300, train_loss: 3.8215\n",
      "234/300, train_loss: 3.7025\n",
      "235/300, train_loss: 6.4430\n",
      "236/300, train_loss: 2.6941\n",
      "237/300, train_loss: 4.8558\n",
      "238/300, train_loss: 0.3475\n",
      "239/300, train_loss: 0.9878\n",
      "240/300, train_loss: 3.1839\n",
      "241/300, train_loss: 1.5602\n",
      "242/300, train_loss: 2.1849\n",
      "243/300, train_loss: 3.8279\n",
      "244/300, train_loss: 3.4764\n",
      "245/300, train_loss: 8.1569\n",
      "246/300, train_loss: 8.7743\n",
      "247/300, train_loss: 1.0933\n",
      "248/300, train_loss: 1.1442\n",
      "249/300, train_loss: 1.3427\n",
      "250/300, train_loss: 4.1059\n",
      "251/300, train_loss: 7.5279\n",
      "252/300, train_loss: 1.8340\n",
      "253/300, train_loss: 8.8402\n",
      "254/300, train_loss: 2.3262\n",
      "255/300, train_loss: 2.3186\n",
      "256/300, train_loss: 6.8872\n",
      "257/300, train_loss: 4.5758\n",
      "258/300, train_loss: 0.4434\n",
      "259/300, train_loss: 1.8855\n",
      "260/300, train_loss: 3.0095\n",
      "261/300, train_loss: 0.4850\n",
      "262/300, train_loss: 6.9887\n",
      "263/300, train_loss: 4.1932\n",
      "264/300, train_loss: 5.6659\n",
      "265/300, train_loss: 1.5952\n",
      "266/300, train_loss: 0.9500\n",
      "267/300, train_loss: 1.6203\n",
      "268/300, train_loss: 7.3891\n",
      "269/300, train_loss: 2.6242\n",
      "270/300, train_loss: 2.2287\n",
      "271/300, train_loss: 6.9947\n",
      "272/300, train_loss: 1.3862\n",
      "273/300, train_loss: 0.7600\n",
      "274/300, train_loss: 2.1145\n",
      "275/300, train_loss: 4.0312\n",
      "276/300, train_loss: 2.9624\n",
      "277/300, train_loss: 3.6784\n",
      "278/300, train_loss: 4.3892\n",
      "279/300, train_loss: 1.2785\n",
      "280/300, train_loss: 2.7144\n",
      "281/300, train_loss: 6.0949\n",
      "282/300, train_loss: 2.5373\n",
      "283/300, train_loss: 4.4989\n",
      "284/300, train_loss: 0.1145\n",
      "285/300, train_loss: 4.5860\n",
      "286/300, train_loss: 2.4496\n",
      "287/300, train_loss: 4.3976\n",
      "288/300, train_loss: 4.4943\n",
      "289/300, train_loss: 3.4132\n",
      "290/300, train_loss: 1.0800\n",
      "291/300, train_loss: 2.9180\n",
      "292/300, train_loss: 6.9739\n",
      "293/300, train_loss: 4.0869\n",
      "294/300, train_loss: 2.8111\n",
      "295/300, train_loss: 4.0337\n",
      "296/300, train_loss: 1.5859\n",
      "297/300, train_loss: 0.9932\n",
      "298/300, train_loss: 5.7619\n",
      "299/300, train_loss: 4.4489\n",
      "300/300, train_loss: 6.0483\n",
      "epoch 5 average loss: 3.9225\n",
      "----------\n",
      "epoch 6/150\n",
      "1/300, train_loss: 9.7741\n",
      "2/300, train_loss: 2.1724\n",
      "3/300, train_loss: 1.6728\n",
      "4/300, train_loss: 0.8919\n",
      "5/300, train_loss: 3.2165\n",
      "6/300, train_loss: 2.9192\n",
      "7/300, train_loss: 1.0620\n",
      "8/300, train_loss: 0.0467\n",
      "9/300, train_loss: 3.6918\n",
      "10/300, train_loss: 4.9699\n",
      "11/300, train_loss: 3.5594\n",
      "12/300, train_loss: 0.6145\n",
      "13/300, train_loss: 4.9841\n",
      "14/300, train_loss: 6.9079\n",
      "15/300, train_loss: 5.6931\n",
      "16/300, train_loss: 0.4497\n",
      "17/300, train_loss: 6.0549\n",
      "18/300, train_loss: 6.4884\n",
      "19/300, train_loss: 4.8241\n",
      "20/300, train_loss: 4.2598\n",
      "21/300, train_loss: 6.9642\n",
      "22/300, train_loss: 6.3948\n",
      "23/300, train_loss: 3.8380\n",
      "24/300, train_loss: 3.8345\n",
      "25/300, train_loss: 9.0762\n",
      "26/300, train_loss: 2.9859\n",
      "27/300, train_loss: 3.0590\n",
      "28/300, train_loss: 4.6227\n",
      "29/300, train_loss: 1.1475\n",
      "30/300, train_loss: 4.2826\n",
      "31/300, train_loss: 7.9873\n",
      "32/300, train_loss: 0.7898\n",
      "33/300, train_loss: 6.2954\n",
      "34/300, train_loss: 2.7720\n",
      "35/300, train_loss: 1.2826\n",
      "36/300, train_loss: 5.9271\n",
      "37/300, train_loss: 3.8971\n",
      "38/300, train_loss: 2.2776\n",
      "39/300, train_loss: 1.7753\n",
      "40/300, train_loss: 6.0315\n",
      "41/300, train_loss: 1.2718\n",
      "42/300, train_loss: 2.5809\n",
      "43/300, train_loss: 1.8184\n",
      "44/300, train_loss: 5.8166\n",
      "45/300, train_loss: 5.4988\n",
      "46/300, train_loss: 2.2153\n",
      "47/300, train_loss: 5.8015\n",
      "48/300, train_loss: 1.2300\n",
      "49/300, train_loss: 0.1492\n",
      "50/300, train_loss: 2.6919\n",
      "51/300, train_loss: 3.2160\n",
      "52/300, train_loss: 6.7149\n",
      "53/300, train_loss: 1.7238\n",
      "54/300, train_loss: 2.1690\n",
      "55/300, train_loss: 7.3159\n",
      "56/300, train_loss: 2.1594\n",
      "57/300, train_loss: 2.9973\n",
      "58/300, train_loss: 4.3205\n",
      "59/300, train_loss: 1.2493\n",
      "60/300, train_loss: 3.7733\n",
      "61/300, train_loss: 3.9188\n",
      "62/300, train_loss: 3.2464\n",
      "63/300, train_loss: 2.8680\n",
      "64/300, train_loss: 5.8322\n",
      "65/300, train_loss: 7.6258\n",
      "66/300, train_loss: 2.3064\n",
      "67/300, train_loss: 8.2876\n",
      "68/300, train_loss: 9.4598\n",
      "69/300, train_loss: 3.7211\n",
      "70/300, train_loss: 7.2816\n",
      "71/300, train_loss: 1.0797\n",
      "72/300, train_loss: 2.8885\n",
      "73/300, train_loss: 1.8447\n",
      "74/300, train_loss: 1.5050\n",
      "75/300, train_loss: 3.3045\n",
      "76/300, train_loss: 4.2794\n",
      "77/300, train_loss: 5.1745\n",
      "78/300, train_loss: 0.5062\n",
      "79/300, train_loss: 3.4998\n",
      "80/300, train_loss: 5.1260\n",
      "81/300, train_loss: 7.4033\n",
      "82/300, train_loss: 2.1976\n",
      "83/300, train_loss: 6.9650\n",
      "84/300, train_loss: 0.9694\n",
      "85/300, train_loss: 5.4228\n",
      "86/300, train_loss: 2.9731\n",
      "87/300, train_loss: 1.3274\n",
      "88/300, train_loss: 4.6213\n",
      "89/300, train_loss: 4.5303\n",
      "90/300, train_loss: 3.9925\n",
      "91/300, train_loss: 0.5049\n",
      "92/300, train_loss: 4.9776\n",
      "93/300, train_loss: 3.3660\n",
      "94/300, train_loss: 4.6458\n",
      "95/300, train_loss: 5.0534\n",
      "96/300, train_loss: 2.9380\n",
      "97/300, train_loss: 1.9268\n",
      "98/300, train_loss: 1.7135\n",
      "99/300, train_loss: 3.1252\n",
      "100/300, train_loss: 5.6839\n",
      "101/300, train_loss: 1.4869\n",
      "102/300, train_loss: 7.1091\n",
      "103/300, train_loss: 4.5010\n",
      "104/300, train_loss: 3.3791\n",
      "105/300, train_loss: 3.1585\n",
      "106/300, train_loss: 4.7031\n",
      "107/300, train_loss: 0.0808\n",
      "108/300, train_loss: 4.5614\n",
      "109/300, train_loss: 3.9180\n",
      "110/300, train_loss: 6.7459\n",
      "111/300, train_loss: 1.6189\n",
      "112/300, train_loss: 6.2201\n",
      "113/300, train_loss: 0.5412\n",
      "114/300, train_loss: 1.5617\n",
      "115/300, train_loss: 8.3617\n",
      "116/300, train_loss: 1.0314\n",
      "117/300, train_loss: 4.3557\n",
      "118/300, train_loss: 3.4586\n",
      "119/300, train_loss: 2.5118\n",
      "120/300, train_loss: 4.8864\n",
      "121/300, train_loss: 3.5602\n",
      "122/300, train_loss: 8.4533\n",
      "123/300, train_loss: 1.8535\n",
      "124/300, train_loss: 3.4682\n",
      "125/300, train_loss: 1.1094\n",
      "126/300, train_loss: 6.2469\n",
      "127/300, train_loss: 8.3541\n",
      "128/300, train_loss: 1.8443\n",
      "129/300, train_loss: 6.2321\n",
      "130/300, train_loss: 1.9356\n",
      "131/300, train_loss: 6.4733\n",
      "132/300, train_loss: 1.7860\n",
      "133/300, train_loss: 5.8529\n",
      "134/300, train_loss: 7.1154\n",
      "135/300, train_loss: 6.5883\n",
      "136/300, train_loss: 6.4990\n",
      "137/300, train_loss: 6.3195\n",
      "138/300, train_loss: 4.9311\n",
      "139/300, train_loss: 3.8467\n",
      "140/300, train_loss: 3.7960\n",
      "141/300, train_loss: 5.7768\n",
      "142/300, train_loss: 1.6709\n",
      "143/300, train_loss: 2.1622\n",
      "144/300, train_loss: 6.6197\n",
      "145/300, train_loss: 0.1875\n",
      "146/300, train_loss: 2.5040\n",
      "147/300, train_loss: 5.6815\n",
      "148/300, train_loss: 0.1582\n",
      "149/300, train_loss: 3.9773\n",
      "150/300, train_loss: 3.9035\n",
      "151/300, train_loss: 5.4956\n",
      "152/300, train_loss: 6.7823\n",
      "153/300, train_loss: 5.8061\n",
      "154/300, train_loss: 1.3210\n",
      "155/300, train_loss: 2.5355\n",
      "156/300, train_loss: 1.8836\n",
      "157/300, train_loss: 1.4536\n",
      "158/300, train_loss: 0.6935\n",
      "159/300, train_loss: 6.5369\n",
      "160/300, train_loss: 2.0550\n",
      "161/300, train_loss: 1.2664\n",
      "162/300, train_loss: 3.1284\n",
      "163/300, train_loss: 4.9378\n",
      "164/300, train_loss: 3.6934\n",
      "165/300, train_loss: 3.2178\n",
      "166/300, train_loss: 1.8252\n",
      "167/300, train_loss: 1.6946\n",
      "168/300, train_loss: 2.7948\n",
      "169/300, train_loss: 4.5940\n",
      "170/300, train_loss: 7.1029\n",
      "171/300, train_loss: 7.1356\n",
      "172/300, train_loss: 2.2428\n",
      "173/300, train_loss: 1.5508\n",
      "174/300, train_loss: 4.3766\n",
      "175/300, train_loss: 3.2423\n",
      "176/300, train_loss: 2.4056\n",
      "177/300, train_loss: 5.2220\n",
      "178/300, train_loss: 5.0267\n",
      "179/300, train_loss: 2.0301\n",
      "180/300, train_loss: 0.4186\n",
      "181/300, train_loss: 8.5151\n",
      "182/300, train_loss: 0.6777\n",
      "183/300, train_loss: 7.8700\n",
      "184/300, train_loss: 4.2841\n",
      "185/300, train_loss: 3.6634\n",
      "186/300, train_loss: 5.9618\n",
      "187/300, train_loss: 3.5625\n",
      "188/300, train_loss: 0.8485\n",
      "189/300, train_loss: 7.7571\n",
      "190/300, train_loss: 4.4782\n",
      "191/300, train_loss: 8.4246\n",
      "192/300, train_loss: 1.2140\n",
      "193/300, train_loss: 7.6955\n",
      "194/300, train_loss: 4.6763\n",
      "195/300, train_loss: 1.3427\n",
      "196/300, train_loss: 3.9380\n",
      "197/300, train_loss: 4.4579\n",
      "198/300, train_loss: 2.9822\n",
      "199/300, train_loss: 3.2666\n",
      "200/300, train_loss: 4.5019\n",
      "201/300, train_loss: 7.1461\n",
      "202/300, train_loss: 6.9458\n",
      "203/300, train_loss: 7.5647\n",
      "204/300, train_loss: 1.3829\n",
      "205/300, train_loss: 2.0919\n",
      "206/300, train_loss: 1.2849\n",
      "207/300, train_loss: 6.0166\n",
      "208/300, train_loss: 2.1776\n",
      "209/300, train_loss: 3.7124\n",
      "210/300, train_loss: 7.8741\n",
      "211/300, train_loss: 0.0797\n",
      "212/300, train_loss: 4.5484\n",
      "213/300, train_loss: 6.9053\n",
      "214/300, train_loss: 3.1370\n",
      "215/300, train_loss: 3.5637\n",
      "216/300, train_loss: 1.9142\n",
      "217/300, train_loss: 0.8453\n",
      "218/300, train_loss: 0.7346\n",
      "219/300, train_loss: 5.9617\n",
      "220/300, train_loss: 8.0572\n",
      "221/300, train_loss: 4.1050\n",
      "222/300, train_loss: 0.8119\n",
      "223/300, train_loss: 5.7853\n",
      "224/300, train_loss: 1.6707\n",
      "225/300, train_loss: 3.7835\n",
      "226/300, train_loss: 0.9207\n",
      "227/300, train_loss: 3.1821\n",
      "228/300, train_loss: 5.8727\n",
      "229/300, train_loss: 6.7751\n",
      "230/300, train_loss: 1.3737\n",
      "231/300, train_loss: 3.9859\n",
      "232/300, train_loss: 0.8075\n",
      "233/300, train_loss: 6.3377\n",
      "234/300, train_loss: 1.8163\n",
      "235/300, train_loss: 4.2437\n",
      "236/300, train_loss: 7.2557\n",
      "237/300, train_loss: 5.7257\n",
      "238/300, train_loss: 0.8767\n",
      "239/300, train_loss: 1.2782\n",
      "240/300, train_loss: 2.0336\n",
      "241/300, train_loss: 5.9350\n",
      "242/300, train_loss: 0.6611\n",
      "243/300, train_loss: 4.0097\n",
      "244/300, train_loss: 6.3757\n",
      "245/300, train_loss: 2.1421\n",
      "246/300, train_loss: 0.9027\n",
      "247/300, train_loss: 2.8130\n",
      "248/300, train_loss: 0.4458\n",
      "249/300, train_loss: 1.8668\n",
      "250/300, train_loss: 5.1735\n",
      "251/300, train_loss: 3.9445\n",
      "252/300, train_loss: 1.0047\n",
      "253/300, train_loss: 2.6270\n",
      "254/300, train_loss: 1.7985\n",
      "255/300, train_loss: 7.5942\n",
      "256/300, train_loss: 3.7757\n",
      "257/300, train_loss: 1.0563\n",
      "258/300, train_loss: 3.6884\n",
      "259/300, train_loss: 2.1796\n",
      "260/300, train_loss: 5.9777\n",
      "261/300, train_loss: 2.9084\n",
      "262/300, train_loss: 1.5352\n",
      "263/300, train_loss: 3.2465\n",
      "264/300, train_loss: 5.1185\n",
      "265/300, train_loss: 5.7611\n",
      "266/300, train_loss: 6.9921\n",
      "267/300, train_loss: 0.0078\n",
      "268/300, train_loss: 7.7307\n",
      "269/300, train_loss: 5.3122\n",
      "270/300, train_loss: 7.6930\n",
      "271/300, train_loss: 1.6599\n",
      "272/300, train_loss: 7.5859\n",
      "273/300, train_loss: 2.9607\n",
      "274/300, train_loss: 4.9952\n",
      "275/300, train_loss: 1.4839\n",
      "276/300, train_loss: 3.3453\n",
      "277/300, train_loss: 1.1389\n",
      "278/300, train_loss: 6.7700\n",
      "279/300, train_loss: 3.7356\n",
      "280/300, train_loss: 6.0209\n",
      "281/300, train_loss: 9.1878\n",
      "282/300, train_loss: 5.1734\n",
      "283/300, train_loss: 0.9271\n",
      "284/300, train_loss: 3.5434\n",
      "285/300, train_loss: 10.1884\n",
      "286/300, train_loss: 5.6379\n",
      "287/300, train_loss: 1.0629\n",
      "288/300, train_loss: 5.8559\n",
      "289/300, train_loss: 8.8380\n",
      "290/300, train_loss: 0.6041\n",
      "291/300, train_loss: 7.2307\n",
      "292/300, train_loss: 3.1365\n",
      "293/300, train_loss: 4.8859\n",
      "294/300, train_loss: 3.4482\n",
      "295/300, train_loss: 0.1545\n",
      "296/300, train_loss: 6.3323\n",
      "297/300, train_loss: 1.2757\n",
      "298/300, train_loss: 1.0412\n",
      "299/300, train_loss: 2.9079\n",
      "300/300, train_loss: 1.5391\n",
      "epoch 6 average loss: 3.8717\n",
      "Current epoch: 6 current accuracy: 0.0469 \n",
      "Best accuracy: 0.0469 at epoch 2\n",
      "----------\n",
      "epoch 7/150\n",
      "1/300, train_loss: 1.3445\n",
      "2/300, train_loss: 2.5883\n",
      "3/300, train_loss: 6.0481\n",
      "4/300, train_loss: 0.6203\n",
      "5/300, train_loss: 2.3491\n",
      "6/300, train_loss: 3.6614\n",
      "7/300, train_loss: 4.9025\n",
      "8/300, train_loss: 3.3995\n",
      "9/300, train_loss: 10.4761\n",
      "10/300, train_loss: 3.7768\n",
      "11/300, train_loss: 0.0071\n",
      "12/300, train_loss: 2.3225\n",
      "13/300, train_loss: 2.4138\n",
      "14/300, train_loss: 0.7452\n",
      "15/300, train_loss: 3.6273\n",
      "16/300, train_loss: 4.4299\n",
      "17/300, train_loss: 0.2073\n",
      "18/300, train_loss: 3.6263\n",
      "19/300, train_loss: 7.1626\n",
      "20/300, train_loss: 1.2116\n",
      "21/300, train_loss: 2.1230\n",
      "22/300, train_loss: 1.9184\n",
      "23/300, train_loss: 5.4712\n",
      "24/300, train_loss: 2.2603\n",
      "25/300, train_loss: 1.2634\n",
      "26/300, train_loss: 3.1732\n",
      "27/300, train_loss: 4.3743\n",
      "28/300, train_loss: 7.8502\n",
      "29/300, train_loss: 2.5854\n",
      "30/300, train_loss: 0.9827\n",
      "31/300, train_loss: 5.8956\n",
      "32/300, train_loss: 3.6422\n",
      "33/300, train_loss: 3.3102\n",
      "34/300, train_loss: 8.5901\n",
      "35/300, train_loss: 0.9018\n",
      "36/300, train_loss: 3.5840\n",
      "37/300, train_loss: 4.4589\n",
      "38/300, train_loss: 2.4157\n",
      "39/300, train_loss: 1.9061\n",
      "40/300, train_loss: 3.6426\n",
      "41/300, train_loss: 10.3467\n",
      "42/300, train_loss: 1.2589\n",
      "43/300, train_loss: 3.5344\n",
      "44/300, train_loss: 1.4695\n",
      "45/300, train_loss: 1.2479\n",
      "46/300, train_loss: 5.9302\n",
      "47/300, train_loss: 1.9435\n",
      "48/300, train_loss: 2.6199\n",
      "49/300, train_loss: 3.6885\n",
      "50/300, train_loss: 2.6398\n",
      "51/300, train_loss: 1.2446\n",
      "52/300, train_loss: 2.8191\n",
      "53/300, train_loss: 7.7118\n",
      "54/300, train_loss: 3.3283\n",
      "55/300, train_loss: 0.9659\n",
      "56/300, train_loss: 7.9370\n",
      "57/300, train_loss: 4.9079\n",
      "58/300, train_loss: 5.7486\n",
      "59/300, train_loss: 5.3147\n",
      "60/300, train_loss: 1.5104\n",
      "61/300, train_loss: 3.6002\n",
      "62/300, train_loss: 0.7917\n",
      "63/300, train_loss: 3.8515\n",
      "64/300, train_loss: 6.9945\n",
      "65/300, train_loss: 5.8869\n",
      "66/300, train_loss: 1.3356\n",
      "67/300, train_loss: 9.3971\n",
      "68/300, train_loss: 6.0508\n",
      "69/300, train_loss: 3.3251\n",
      "70/300, train_loss: 3.6543\n",
      "71/300, train_loss: 7.1952\n",
      "72/300, train_loss: 4.5282\n",
      "73/300, train_loss: 5.0510\n",
      "74/300, train_loss: 1.6569\n",
      "75/300, train_loss: 2.2152\n",
      "76/300, train_loss: 3.8524\n",
      "77/300, train_loss: 5.3101\n",
      "78/300, train_loss: 3.1521\n",
      "79/300, train_loss: 0.1203\n",
      "80/300, train_loss: 1.6691\n",
      "81/300, train_loss: 1.4917\n",
      "82/300, train_loss: 7.3972\n",
      "83/300, train_loss: 5.2051\n",
      "84/300, train_loss: 5.0949\n",
      "85/300, train_loss: 5.0658\n",
      "86/300, train_loss: 1.4403\n",
      "87/300, train_loss: 4.2366\n",
      "88/300, train_loss: 2.8145\n",
      "89/300, train_loss: 5.7997\n",
      "90/300, train_loss: 6.4367\n",
      "91/300, train_loss: 6.9425\n",
      "92/300, train_loss: 5.5219\n",
      "93/300, train_loss: 2.9064\n",
      "94/300, train_loss: 2.3189\n",
      "95/300, train_loss: 2.0757\n",
      "96/300, train_loss: 1.8438\n",
      "97/300, train_loss: 1.7680\n",
      "98/300, train_loss: 5.0807\n",
      "99/300, train_loss: 6.2690\n",
      "100/300, train_loss: 8.5997\n",
      "101/300, train_loss: 3.3313\n",
      "102/300, train_loss: 5.4831\n",
      "103/300, train_loss: 1.6145\n",
      "104/300, train_loss: 4.3932\n",
      "105/300, train_loss: 2.3229\n",
      "106/300, train_loss: 0.1437\n",
      "107/300, train_loss: 1.4103\n",
      "108/300, train_loss: 4.9725\n",
      "109/300, train_loss: 4.5095\n",
      "110/300, train_loss: 3.4220\n",
      "111/300, train_loss: 4.8494\n",
      "112/300, train_loss: 5.3805\n",
      "113/300, train_loss: 8.0166\n",
      "114/300, train_loss: 0.8796\n",
      "115/300, train_loss: 6.9639\n",
      "116/300, train_loss: 3.6497\n",
      "117/300, train_loss: 3.4050\n",
      "118/300, train_loss: 2.5659\n",
      "119/300, train_loss: 1.7986\n",
      "120/300, train_loss: 0.8782\n",
      "121/300, train_loss: 0.5966\n",
      "122/300, train_loss: 3.8220\n",
      "123/300, train_loss: 3.0262\n",
      "124/300, train_loss: 5.8672\n",
      "125/300, train_loss: 2.2330\n",
      "126/300, train_loss: 5.6093\n",
      "127/300, train_loss: 3.0149\n",
      "128/300, train_loss: 1.1814\n",
      "129/300, train_loss: 9.4664\n",
      "130/300, train_loss: 3.9413\n",
      "131/300, train_loss: 5.9852\n",
      "132/300, train_loss: 5.8061\n",
      "133/300, train_loss: 4.3839\n",
      "134/300, train_loss: 4.4920\n",
      "135/300, train_loss: 3.4957\n",
      "136/300, train_loss: 0.8854\n",
      "137/300, train_loss: 4.9900\n",
      "138/300, train_loss: 1.3651\n",
      "139/300, train_loss: 5.4477\n",
      "140/300, train_loss: 0.3049\n",
      "141/300, train_loss: 0.9941\n",
      "142/300, train_loss: 0.4259\n",
      "143/300, train_loss: 5.8214\n",
      "144/300, train_loss: 4.0237\n",
      "145/300, train_loss: 10.1909\n",
      "146/300, train_loss: 1.7066\n",
      "147/300, train_loss: 2.0529\n",
      "148/300, train_loss: 6.1766\n",
      "149/300, train_loss: 3.0149\n",
      "150/300, train_loss: 2.3601\n",
      "151/300, train_loss: 3.4245\n",
      "152/300, train_loss: 1.1624\n",
      "153/300, train_loss: 4.2595\n",
      "154/300, train_loss: 7.0804\n",
      "155/300, train_loss: 1.7021\n",
      "156/300, train_loss: 0.0611\n",
      "157/300, train_loss: 5.1857\n",
      "158/300, train_loss: 1.9243\n",
      "159/300, train_loss: 4.0642\n",
      "160/300, train_loss: 0.4945\n",
      "161/300, train_loss: 5.0045\n",
      "162/300, train_loss: 4.8014\n",
      "163/300, train_loss: 1.3414\n",
      "164/300, train_loss: 2.4543\n",
      "165/300, train_loss: 2.4098\n",
      "166/300, train_loss: 4.3561\n",
      "167/300, train_loss: 4.9519\n",
      "168/300, train_loss: 8.0228\n",
      "169/300, train_loss: 2.8496\n",
      "170/300, train_loss: 5.4744\n",
      "171/300, train_loss: 1.7336\n",
      "172/300, train_loss: 4.5141\n",
      "173/300, train_loss: 3.1707\n",
      "174/300, train_loss: 1.2556\n",
      "175/300, train_loss: 3.1888\n",
      "176/300, train_loss: 1.4170\n",
      "177/300, train_loss: 4.7134\n",
      "178/300, train_loss: 3.8297\n",
      "179/300, train_loss: 4.1834\n",
      "180/300, train_loss: 2.1292\n",
      "181/300, train_loss: 4.3056\n",
      "182/300, train_loss: 2.7197\n",
      "183/300, train_loss: 9.0648\n",
      "184/300, train_loss: 2.6250\n",
      "185/300, train_loss: 1.4855\n",
      "186/300, train_loss: 7.0812\n",
      "187/300, train_loss: 3.9948\n",
      "188/300, train_loss: 1.8725\n",
      "189/300, train_loss: 7.4336\n",
      "190/300, train_loss: 7.4774\n",
      "191/300, train_loss: 5.3354\n",
      "192/300, train_loss: 9.5172\n",
      "193/300, train_loss: 5.7744\n",
      "194/300, train_loss: 5.6478\n",
      "195/300, train_loss: 3.8107\n",
      "196/300, train_loss: 2.7390\n",
      "197/300, train_loss: 3.3270\n",
      "198/300, train_loss: 4.1781\n",
      "199/300, train_loss: 0.2509\n",
      "200/300, train_loss: 4.2911\n",
      "201/300, train_loss: 0.8597\n",
      "202/300, train_loss: 3.3243\n",
      "203/300, train_loss: 2.6113\n",
      "204/300, train_loss: 1.3458\n",
      "205/300, train_loss: 3.9417\n",
      "206/300, train_loss: 5.2187\n",
      "207/300, train_loss: 4.8957\n",
      "208/300, train_loss: 4.1819\n",
      "209/300, train_loss: 3.2302\n",
      "210/300, train_loss: 5.7957\n",
      "211/300, train_loss: 6.2634\n",
      "212/300, train_loss: 2.6902\n",
      "213/300, train_loss: 3.6065\n",
      "214/300, train_loss: 5.7856\n",
      "215/300, train_loss: 4.6285\n",
      "216/300, train_loss: 5.1970\n",
      "217/300, train_loss: 3.5927\n",
      "218/300, train_loss: 5.7929\n",
      "219/300, train_loss: 2.0734\n",
      "220/300, train_loss: 4.3571\n",
      "221/300, train_loss: 3.8748\n",
      "222/300, train_loss: 0.6368\n",
      "223/300, train_loss: 1.6235\n",
      "224/300, train_loss: 1.9324\n",
      "225/300, train_loss: 0.4120\n",
      "226/300, train_loss: 4.0955\n",
      "227/300, train_loss: 0.2503\n",
      "228/300, train_loss: 5.2776\n",
      "229/300, train_loss: 3.2489\n",
      "230/300, train_loss: 2.8059\n",
      "231/300, train_loss: 0.8379\n",
      "232/300, train_loss: 0.8865\n",
      "233/300, train_loss: 9.3218\n",
      "234/300, train_loss: 1.6306\n",
      "235/300, train_loss: 2.1145\n",
      "236/300, train_loss: 2.1668\n",
      "237/300, train_loss: 1.3078\n",
      "238/300, train_loss: 6.2939\n",
      "239/300, train_loss: 3.1718\n",
      "240/300, train_loss: 2.8222\n",
      "241/300, train_loss: 5.4236\n",
      "242/300, train_loss: 3.0467\n",
      "243/300, train_loss: 0.9275\n",
      "244/300, train_loss: 3.9137\n",
      "245/300, train_loss: 3.1970\n",
      "246/300, train_loss: 0.1038\n",
      "247/300, train_loss: 2.9242\n",
      "248/300, train_loss: 3.2494\n",
      "249/300, train_loss: 5.7741\n",
      "250/300, train_loss: 1.1708\n",
      "251/300, train_loss: 1.5025\n",
      "252/300, train_loss: 3.7293\n",
      "253/300, train_loss: 3.6346\n",
      "254/300, train_loss: 4.8854\n"
     ]
    }
   ],
   "source": [
    "# Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "# model = monai.networks.nets.DenseNet264(spatial_dims=3, in_channels=1, out_channels=2, growth_rate = 40).to(device)\n",
    "\n",
    "model = monai.networks.nets.EfficientNetBN('efficientnet-b3', spatial_dims=3, in_channels=1, num_classes=NumClasses).to(device)\n",
    "\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# %%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "\n",
    "# loss_function = torch.nn.CrossEntropyLoss()\n",
    "# loss_function = torch.nn.BCEWithLogitsLoss()  # also works with this data\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "writer = SummaryWriter(modelName)\n",
    "max_epochs = 150\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"epochs\": max_epochs,\n",
    "  \"batch_size\": batch_size\n",
    "}\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        output = Variable(outputs, requires_grad=True)\n",
    "        target = Variable(labels, requires_grad=True)\n",
    "        loss = criterion(output, target)\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "        # Optional\n",
    "        wandb.watch(model)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "\n",
    "        num_correct = 0.0\n",
    "        metric_count = 0\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_images)\n",
    "                value = torch.eq(val_outputs.argmax(dim=1), val_labels.argmax(dim=1))\n",
    "                metric_count += len(value)\n",
    "                num_correct += value.sum().item()\n",
    "\n",
    "        metric = num_correct / metric_count\n",
    "        metric_values.append(metric)\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"best_metric_model_classification3d_array.pth\")\n",
    "            modelPickle = os.path.join(RunFolder, modelName + time.strftime(\"_%Y%m%d_%H%M%S\") + '.pickle')\n",
    "            if not os.path.exists(RunFolder):\n",
    "                os.makedirs(RunFolder)\n",
    "            \n",
    "            with open(modelPickle, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            f.close()\n",
    "            print(\"saved new best metric model = {}\".format(modelPickle))\n",
    "\n",
    "        print(f\"Current epoch: {epoch+1} current accuracy: {metric:.4f} \")\n",
    "        print(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "        writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occlusion sensitivity\n",
    "One method for trying to visualise why the network made a given prediction is occlusion sensitivity. We occlude part of the image, and see how the probability of a given prediction changes. We then iterate over the image, moving the occluded portion as we go, and in doing so we build up a sensitivity map detailing which areas were the most important in making the decision.\n",
    "\n",
    "#### Bounds\n",
    "If we were to test the occlusion centred on all voxels in our image, we would have to do `torch.prod(im.shape) = 96^3 = ~1e6` predictions. We can use the bounding box to only to the estimations in a region of interest, for example over one slice.\n",
    "\n",
    "To do this, we simply give the bounding box as `(minC,maxC,minD,maxD,minH,maxH,minW,maxW)`. We can use `-1` for any value to use its full extent (`0` and `im.shape-1` for min's and max's, respectively).\n",
    "\n",
    "#### Output\n",
    "The output image in this example will look fairly bad, since our network hasn't been trained for very long. Training for longer should improve the quality of the occlusion map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "On given data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "modelPickle = \"/HippoDense/DenseNet264_40/DenseNet264_40_20220524_043402.pickle\"\n",
    "savedModel = open(modelPickle, 'rb')\n",
    "trainedModel = pickle.load(savedModel)\n",
    "savedModel.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a validation data loader\n",
    "test_ds = ImageDataset(image_files=images[-10:], labels=labels[-10:], transform=val_transforms)\n",
    "test_loader = DataLoader(val_ds, batch_size=1, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "itera = iter(test_loader)\n",
    "\n",
    "\n",
    "def get_next_im():\n",
    "    test_data = next(itera)\n",
    "    return test_data[0].to(device), test_data[1].unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def plot_occlusion_heatmap(im, heatmap):\n",
    "    plt.subplots(1, 2)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.squeeze(im.cpu()))\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainedModel = Network(input_size, output_size) \n",
    "path = trainedModel \n",
    "model.load_state_dict(torch.load(path)) \n",
    "    \n",
    "running_accuracy = 0 \n",
    "total = 0 \n",
    "\n",
    "with torch.no_grad(): \n",
    "    for data in test_loader: \n",
    "        inputs, outputs = data \n",
    "        outputs = outputs.to(torch.float32) \n",
    "        predicted_outputs = model(inputs) \n",
    "        _, predicted = torch.max(predicted_outputs, 1) \n",
    "        total += outputs.size(0) \n",
    "        running_accuracy += (predicted == outputs).sum().item() \n",
    "\n",
    "    print('Accuracy of the model based on the test set of', test_split ,'inputs is: %d %%' % (100 * running_accuracy / total))    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for image 1 is 0.07596635818481445 sec\n",
      "Execution Time for image 2 is 0.07570838928222656 sec\n",
      "Execution Time for image 3 is 0.07555842399597168 sec\n",
      "Execution Time for image 4 is 0.07501721382141113 sec\n",
      "Execution Time for image 5 is 0.0752708911895752 sec\n",
      "Execution Time for image 6 is 0.07816457748413086 sec\n",
      "Execution Time for image 7 is 0.07525372505187988 sec\n",
      "Execution Time for image 8 is 0.07520079612731934 sec\n",
      "Execution Time for image 9 is 0.07532620429992676 sec\n",
      "Execution Time for image 10 is 0.07719779014587402 sec\n",
      "Execution Time for image 11 is 0.07613801956176758 sec\n",
      "Execution Time for image 12 is 0.07541465759277344 sec\n",
      "Execution Time for image 13 is 0.07521367073059082 sec\n",
      "Execution Time for image 14 is 0.07511568069458008 sec\n",
      "Execution Time for image 15 is 0.07594466209411621 sec\n",
      "Execution Time for image 16 is 0.07702970504760742 sec\n",
      "Execution Time for image 17 is 0.07524585723876953 sec\n",
      "Execution Time for image 18 is 0.07546615600585938 sec\n",
      "Execution Time for image 19 is 0.07518386840820312 sec\n",
      "Execution Time for image 20 is 0.07727241516113281 sec\n",
      "Execution Time for image 21 is 0.07631468772888184 sec\n",
      "Execution Time for image 22 is 0.07729339599609375 sec\n",
      "Execution Time for image 23 is 0.07574677467346191 sec\n",
      "Execution Time for image 24 is 0.07550454139709473 sec\n",
      "Execution Time for image 25 is 0.07555174827575684 sec\n",
      "Execution Time for image 26 is 0.07533621788024902 sec\n",
      "Execution Time for image 27 is 0.07556438446044922 sec\n",
      "Execution Time for image 28 is 0.07522869110107422 sec\n",
      "Execution Time for image 29 is 0.07680010795593262 sec\n",
      "Execution Time for image 30 is 0.07519936561584473 sec\n",
      "Execution Time for image 31 is 0.07553243637084961 sec\n",
      "Execution Time for image 32 is 0.07532715797424316 sec\n",
      "Execution Time for image 33 is 0.07484960556030273 sec\n",
      "Execution Time for image 34 is 0.07541680335998535 sec\n",
      "Execution Time for image 35 is 0.07632613182067871 sec\n",
      "Execution Time for image 36 is 0.07687807083129883 sec\n",
      "Execution Time for image 37 is 0.07527351379394531 sec\n",
      "Execution Time for image 38 is 0.07644939422607422 sec\n",
      "Execution Time for image 39 is 0.07512235641479492 sec\n",
      "Execution Time for image 40 is 0.07533454895019531 sec\n",
      "Execution Time for image 41 is 0.07542109489440918 sec\n",
      "Execution Time for image 42 is 0.07562780380249023 sec\n",
      "Execution Time for image 43 is 0.07534074783325195 sec\n",
      "Execution Time for image 44 is 0.07569551467895508 sec\n",
      "Execution Time for image 45 is 0.07534146308898926 sec\n",
      "Execution Time for image 46 is 0.07517290115356445 sec\n",
      "Execution Time for image 47 is 0.07531166076660156 sec\n",
      "Execution Time for image 48 is 0.07523536682128906 sec\n",
      "Execution Time for image 49 is 0.07548856735229492 sec\n",
      "Execution Time for image 50 is 0.07631874084472656 sec\n",
      "Execution Time for image 51 is 0.07540535926818848 sec\n",
      "Accuracy of the model based on the test set of 30 inputs is: 100 %\n",
      "Accuracy Min =  18.725099601593627\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "metric_count = -1\n",
    "num_correct = 0\n",
    "numImages = 251\n",
    "\n",
    "running_accuracy = 0 \n",
    "total = 0 \n",
    "trainedModel.eval()\n",
    "i = 1\n",
    "for val_data in val_loader:\n",
    "    timestart = time.time()\n",
    "    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "    with torch.no_grad():\n",
    "        val_outputs = trainedModel(val_images)\n",
    "        value = torch.eq(val_outputs.argmax(dim=1), val_labels.argmax(dim=1))\n",
    "        metric_count += len(value)\n",
    "        num_correct += value.sum().item()\n",
    "        # print(val_outputs)\n",
    "        \n",
    "        predicted_outputs = trainedModel(val_images)\n",
    "        _ , predicted = torch.max(predicted_outputs, 1) \n",
    "        total += val_labels.size(0) \n",
    "        running_accuracy += (predicted == val_labels).sum().item() \n",
    "        print(\"Execution Time for image {} is {} sec\".format(i, (time.time() - timestart)))\n",
    "        i += 1\n",
    "\n",
    "print('Accuracy of the model based on the test set of', 30 ,'inputs is: %d %%' % (100 * running_accuracy / total))\n",
    "print(\"Accuracy Min = \", (100 * num_correct /numImages ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256, 208]) torch.Size([1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "tt = Compose([ScaleIntensity()])\n",
    "t_ds = ImageDataset(image_files=images, labels=labels)\n",
    "t_loader = DataLoader(t_ds, batch_size=1)\n",
    "iterat = iter(t_loader)\n",
    "\n",
    "\n",
    "def get_next_im():\n",
    "    t_data = next(iterat)\n",
    "    return t_data[0].to(device), t_data[1].unsqueeze(0).to(device)\n",
    "imgT, labelT = get_next_im()\n",
    "print(imgT.shape, labelT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/HippoDense/DenseNet256_40.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6164436c617373696669636174696f6e486970706f44222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/HippoDense/DenseNet256_40.ipynb#ch0000017vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6164436c617373696669636174696f6e486970706f44222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/HippoDense/DenseNet256_40.ipynb#ch0000017vscode-remote?line=2'>3</a>\u001b[0m \u001b[39m# pathNifti = \"/home/usman/Desktop/Datasets/dldirect_results/seg_Right-Hippocampus.nii.gz\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6164436c617373696669636174696f6e486970706f44222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/HippoDense/DenseNet256_40.ipynb#ch0000017vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# img = image.load_img(pathNifti)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6164436c617373696669636174696f6e486970706f44222c2273657474696e6773223a7b22686f7374223a227373683a2f2f3130362e3234302e3233342e313135227d7d/HippoDense/DenseNet256_40.ipynb#ch0000017vscode-remote?line=4'>5</a>\u001b[0m plotting\u001b[39m.\u001b[39;49mplot_glass_brain(imgT)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py:1030\u001b[0m, in \u001b[0;36mplot_glass_brain\u001b[0;34m(stat_map_img, output_file, display_mode, colorbar, cbar_tick_format, figure, axes, title, threshold, annotate, black_bg, cmap, alpha, vmin, vmax, plot_abs, symmetric_cbar, resampling_interpolation, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py?line=1026'>1027</a>\u001b[0m \u001b[39mif\u001b[39;00m cmap \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py?line=1027'>1028</a>\u001b[0m     cmap \u001b[39m=\u001b[39m cm\u001b[39m.\u001b[39mcold_hot \u001b[39mif\u001b[39;00m black_bg \u001b[39melse\u001b[39;00m cm\u001b[39m.\u001b[39mcold_white_hot\n\u001b[0;32m-> <a href='file:///opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py?line=1029'>1030</a>\u001b[0m \u001b[39mif\u001b[39;00m stat_map_img:\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py?line=1030'>1031</a>\u001b[0m     stat_map_img \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mcheck_niimg_3d(stat_map_img, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/nilearn/plotting/img_plotting.py?line=1031'>1032</a>\u001b[0m     \u001b[39mif\u001b[39;00m plot_abs:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from nilearn import plotting, image\n",
    "import numpy as np\n",
    "# pathNifti = \"/home/usman/Desktop/Datasets/dldirect_results/seg_Right-Hippocampus.nii.gz\"\n",
    "# img = image.load_img(pathNifti)\n",
    "plotting.plot_glass_brain(imgT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = plotting.view_img_on_surf(img, threshold='80%', cmap=nilearn_cmaps['cold_hot'])\n",
    "# view.open_in_browser()\n",
    "\n",
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing occlusion sensitivity: 100%|██████████| 64/64 [00:00<00:00, 228.46it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABX0AAANHCAYAAABq3v4xAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABqKUlEQVR4nOz9fZCd9Xkf/r8PkhAI8yQhIWlFeNoVtgXy1lrFshO71GItY1KBMOXBDpLBRL/a7rj2TFOrQywP7mgsxpm646ExpdJEi4ciFGIjOw2yDObBbhu2m3adFOpm7UCMxLISkkAEsKRdn98ffLPN5txHOvIi7uXW6zVzZnSu+/O5z+fsLp49l6+9rlq9Xq8HAAAAAIBKOKHsAwAAAAAA8MaR9AUAAAAAqBBJXwAAAACACpH0BQAAAACoEElfAAAAAIAKkfQFAAAAAKgQSV8AgHHatm1bLrroorS3t2f9+vUN1x9//PG8+93vzuTJk3P//fePudbT05OOjo50dHSkp6dnNP7nf/7nueSSS9Le3p7Pfvazqdfrx/x9AAAA1SDpCwAwDiMjI/nMZz6TBx98ME899VTuvffePPXUU2PW/Nqv/Vo2bdqUj33sY2Pie/fuzW233ZYnnngivb29ue2227Jv374kyac+9an8p//0nzIwMJCBgYFs27btTXtPAADAW5ukLwDAOPT29qa9vT0XXHBBTjzxxFx//fXZunXrmDXnnXdeFi5cmBNOGPur1/e+9710d3dn+vTpOfPMM9Pd3Z1t27ZlcHAw+/fvz5IlS1Kr1bJy5co88MADb+K7AgAA3soml30AAOD49eEPfzgvvPBC2cc4rNdeey0nn3zy6PPVq1dn9erVo8937tyZc845Z/T5vHnz8sQTT7R076K9O3fuzM6dOzNv3ryGOAAATHTttVpeLfsQR7Bw2bLK/yWdpC8AUJoXXnghfX19ZR/jsLq6uib8GQEAYKJ4Ncn/r+xDHMF3J3jhyRtBewcAgHFoa2vLs88+O/p8x44daWtrG9fetra27Nix41e6JwAAgKQvAMA4LF68OAMDA3n66adz8ODBbN68OcuXL29p77Jly7J9+/bs27cv+/bty/bt27Ns2bLMmTMnp512Wv7sz/4s9Xo9d999d6688spj/E4AAICq0N4BAChVvV4v+wjjMnny5Nxxxx1ZtmxZRkZGcvPNN2fBggVZu3Zturq6snz58vyP//E/smLFiuzbty/f/e5386UvfSlPPvlkpk+fni9+8YtZvHhxkmTt2rWZPn16kuQP/uAP8olPfCKvvfZaLr/88lx++eVlvk0AAGhJLRKOE0Gt/lb/pAUAvGV1dXXlf/yP/1H2MQ5r8eLFevoCAECL2mq1fLrsQxzBtxctqvzv+No7AAAAAABUiGprAKBU/ugIAACqo5ZkStmHQKUvAAAAAECVSPoCAAAAAFSI9g4AQKm0dwAAgOqoRcJxIlDpCwAAAABQIZK+AAAAAAAVotoaAChNvV7X3gEAACqklmRK2YdApS8AAAAAQJVI+gIAAAAAVIikLwBQqr9r8TBRHwAAQOtqeb2f7ER+jMfevXvT3d2djo6OdHd3Z9++fQ1r+vv78973vjcLFizIwoULc999941e+8QnPpHzzz8/nZ2d6ezsTH9/f5LknnvuycKFC3PJJZfkfe97X3784x+P7jnvvPNyySWXpLOzM11dXS2dU9IXAAAAAKAF69evz9KlSzMwMJClS5dm/fr1DWumTZuWu+++O08++WS2bduWz33uc3nxxRdHr3/1q19Nf39/+vv709nZmSQ5//zz89hjj+Uv//Iv88UvfjGrV68ec89HHnkk/f396evra+mckr4AAAAAAC3YunVrVq1alSRZtWpVHnjggYY18+fPT0dHR5Jk7ty5mTVrVnbv3n3Y+77vfe/LmWeemSRZsmRJduzYMa5zSvoCAAAAALRgaGgoc+bMSZLMnj07Q0NDh13f29ubgwcP5sILLxyN3XrrrVm4cGE+//nP58CBAw17Nm7cmMsvv3z0ea1Wy4c+9KEsWrQod911V0vnHG8bCwCAcdE3FwAAqqOWZErZhziC3bt3j+mNu3r16jHtFC677LI8//zzDfvWrVs35nmtVkutVmv6OoODg7nxxhvT09OTE054vfb2K1/5SmbPnp2DBw9m9erVuf3227N27drRPY888kg2btyYH/3oR6OxH/3oR2lra8uuXbvS3d2dt7/97fnABz5w2Pco6QsAAAAAHDdmzpx52N64Dz30UNNrZ599dgYHBzNnzpwMDg5m1qxZhev279+fK664IuvWrcuSJUtG439XJTx16tTcdNNN+f3f//3Ra3/xF3+RW265JQ8++GBmzJgxGm9ra0uSzJo1KytWrEhvb+8Rk77aOwAAAAAAtGD58uXp6elJkvT09OTKK69sWHPw4MGsWLEiK1euzDXXXDPm2uDgYJLX/+LxgQceyMUXX5wk+fnPf56rr7463/zmNzN//vzR9a+88kpefvnl0X9v3759dM/hqPQFAEqlvQMAAFRHLdVOOK5ZsybXXnttNm7cmHPPPTdbtmxJkvT19eXOO+/Mhg0bsmXLljz++OPZs2dPNm3alCTZtGlTOjs78/GPfzy7d+9OvV5PZ2dn7rzzziTJl7/85ezZsyef/vSnkySTJ09OX19fhoaGsmLFiiTJ8PBwPvaxj+XDH/7wEc9Zq/ukBQCUZNGiRflv/+2/lX2Mw/qN3/iNw/7pFwAA8P+cX6vlS2Uf4gjuWLSo8r/ja+8AAAAAAFAhVa62BgDeAvzREQAAVEctyZSyD4FKXwAAAACAKpH0BQAAAACoEO0dAIDS1Ot17R0AAKBCapFwnAhU+gIAAAAAVIikLwAAAABAhai2BgBKpb0DAABURy3JlLIPgUpfAAAAAIAqkfQFAAAAAKgQ7R0AgFJp7wAAANWhvcPEoNIXAAAAAKBCJH0BAAAAACpE0hcAAAAAoEL09AUASqWnLwAAVIuEY/lU+gIAAAAAVIikLwAAAABAhai2BgBKpb0DAABURy3JlLIPgUpfAAAAAIAqkfQFAAAAAKgQ7R0AgNLU63XtHQAAoEJqkXCcCFT6AgAAAABUiKQvAAAAAECFqLYGAEqlvQMAAFRHLcmUsg+BSl8AAAAAgCqR9AUAAAAAqBDtHQCAUmnvAAAA1VGLhONEoNIXAAAAAKBCJH0BAAAAACpE0hcAAAAAoEK02AAASqWnLwAAVEctyZSyD4FKXwAAAACAKpH0BQAAAACoEO0dAIDS1Ot17R0AAKBCapFwnAhU+gIAAAAAVIikLwAAAABAhai2BgBKpb0DAABURy3JlLIPgUpfAAAAAIAqkfQFAAAAAKgQ7R0AgFJp7wAAANVRi4TjRKDSFwAAAACgQiR9AQAAAAAqRLU1AFAq7R0AAKA6akmmlH0IVPoCAAAAAFSJpC8AAAAAQIVo7wAAlEp7BwAAqA7tHSYGlb4AAAAAABUi6QsAAAAAUCGSvgAAAAAAFaKnLwBQmnq9rqcvAABUjIRj+VT6AgAAAABUiKQvAAAAAECFqLYGAEqlvQMAAFRHLckUGcfSqfQFAAAAAKgQSV8AAAAAgApRbA0AlEp7BwAAqI5aLZks41g6lb4AAAAAABUi6QsAAAAAUCGKrQGAUmnvAAAA1VGrJVMmlX0KVPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqI5aLZks41g6lb4AAAAAABUi6QsAAAAAUCGSvgAAAAAAFaLDBgBQmnq9rqcvAABUSC3JFBnH0qn0BQAAAACoEElfAAAAAIAKUWwNAJRKewcAAKiQWpJJZR8Clb4AAAAAABUi6QsAAAAAUCHaOwAApdLeAQAAKqQWGccJQKUvAAAAAECFSPoCAAAAAFSIpC8AUKp6vT6hH63Ytm1bLrroorS3t2f9+vUN1w8cOJDrrrsu7e3tec973pNnnnkmSXLPPfeks7Nz9HHCCSekv78/SXLppZfmoosuGr22a9euN+pLDgAAx87ftXeYyI/jgKQvAMA4jIyM5DOf+UwefPDBPPXUU7n33nvz1FNPjVmzcePGnHnmmfnpT3+az3/+8/nCF76QJPn4xz+e/v7+9Pf355vf/GbOP//8dHZ2ju675557Rq/PmjXrzXxbAADAW5ikLwDAOPT29qa9vT0XXHBBTjzxxFx//fXZunXrmDVbt27NqlWrkiTXXHNNHn744YYq4nvvvTfXX3/9m3ZuAACguo6TgmYAYKJqtYVCWXbv3p2urq7R56tXr87q1atHn+/cuTPnnHPO6PN58+bliSeeGHOPv79m8uTJOf3007Nnz56cddZZo2vuu+++hmTxTTfdlEmTJuWjH/1ofu/3fi+1Wu0NfW8AAHBMyDiWzrcAAOAwZs6cmb6+vmP6Gk888USmTZuWiy++eDR2zz33pK2tLS+//HI++tGP5pvf/GZWrlx5TM8BAABUg/YOAADj0NbWlmeffXb0+Y4dO9LW1tZ0zfDwcF566aXMmDFj9PrmzZtzww03NOxJklNPPTUf+9jH0tvbe6zeAgAAUDGSvgAA47B48eIMDAzk6aefzsGDB7N58+YsX758zJrly5enp6cnSXL//ffngx/84Girhl/+8pfZsmXLmH6+w8PDeeGFF5Ikhw4dyp/8yZ+MqQIGAAA4HO0dAIDS1Ov1Cd/T90gmT56cO+64I8uWLcvIyEhuvvnmLFiwIGvXrk1XV1eWL1+eT37yk7nxxhvT3t6e6dOnZ/PmzaP7H3/88Zxzzjm54IILRmMHDhzIsmXLcujQoYyMjOSyyy7L7/zO75Tx9gAA4OjUkkwq+xDU6m/1T1oAwFvWJZdcku985ztlH+Ow/tk/+2fHvKcvAABURddJtfT9WtmnOLyu0xZV/nd87R0AAAAAACpEewcAoFT+6AgAACqkFhnHCUClLwAAAABAhUj6AgAAAABUiKQvAFCqer0+oR8AAMBR+Lv2DhP5MQ579+5Nd3d3Ojo60t3dnX379jWs6e/vz3vf+94sWLAgCxcuzH333Td67ROf+ETOP//8dHZ2prOzM/39/UmSRx99NKeffvpo/Mtf/vLonm3btuWiiy5Ke3t71q9f39I5JX0BAAAAAFqwfv36LF26NAMDA1m6dGlhEnbatGm5++678+STT2bbtm353Oc+lxdffHH0+le/+tX09/env78/nZ2do/H3v//9o/G1a9cmSUZGRvKZz3wmDz74YJ566qnce++9eeqpp454TklfAAAAAIAWbN26NatWrUqSrFq1Kg888EDDmvnz56ejoyNJMnfu3MyaNSu7d+/+lV6vt7c37e3tueCCC3LiiSfm+uuvz9atW4+4T9IXAChV2e0btHcAAIA32KSJ/di9e3e6urpGH3fddVfLb21oaChz5sxJksyePTtDQ0OHXd/b25uDBw/mwgsvHI3deuutWbhwYT7/+c/nwIEDo/H//t//e971rnfl8ssvz5NPPpkk2blzZ84555zRNfPmzcvOnTuPeM5xdrEAAAAAAHjrmDlzZvr6+ppev+yyy/L88883xNetWzfmea1WS61Wa3qfwcHB3Hjjjenp6ckJJ7xee/uVr3wls2fPzsGDB7N69ercfvvtWbt2bd797nfnb/7mb/K2t70tf/qnf5qrrroqAwMDv+I7lPQFAAAAABj10EMPNb129tlnZ3BwMHPmzMng4GBmzZpVuG7//v254oorsm7duixZsmQ0/ndVwlOnTs1NN92U3//930+SnHbaaaNrPvKRj+TTn/50XnjhhbS1teXZZ58dvbZjx460tbUd8T1o7wAAlKrs9g3aOwAAwBuoltfLTCfyYxyWL1+enp6eJElPT0+uvPLKhjUHDx7MihUrsnLlylxzzTVjrg0ODiZ5/XPQAw88kIsvvjhJ8vzzz49+/ujt7c0vf/nLzJgxI4sXL87AwECefvrpHDx4MJs3b87y5cuPeE6VvgAAAAAALVizZk2uvfbabNy4Meeee262bNmSJOnr68udd96ZDRs2ZMuWLXn88cezZ8+ebNq0KUmyadOmdHZ25uMf/3h2796der2ezs7O3HnnnUmS+++/P9/4xjcyefLknHzyydm8eXNqtVomT56cO+64I8uWLcvIyEhuvvnmLFiw4IjnrNWVsAAAJbnkkkvyrW99q+xjHNYNN9xw2H5fAADA/9N1Si197yz7FIfXVV9U+d/xVfoCAKXRQgEAACrm79o7TGSHyj7AsaenLwAAAABAhUj6AgAAAABUiKQvAAAAAECFTPQOGwBAxenpCwAAFTOp7AMcgZ6+AAAAAAC8lUj6AgAAAABUiPYOAECptHcAAIAKqUXGcQJQ6QsAAAAAUCGSvgAAAAAAFaLYGgAolfYOAABQIdo7TAgqfQEAAAAAKkTSFwAAAACgQhRbAwCl0t4BAAAqRHuHCUGlLwAAAABAhUj6AgAAAABUiGJrAKA09XpdewcAAKiaSWUfAJW+AAAAAAAVIukLAAAAAFAhkr4AAAAAABWipy8AUCo9fQEAoEJqkXGcAFT6AgAAAABUiKQvAAAAAECFKLYGAEqlvQMAAFSI9g4TgkpfAAAAAIAKkfQFAAAAAKgQxdYAQKm0dwAAgAqpJZlU9iFQ6QsAAAAAUCGSvgAAAAAAFaK9AwBQKu0dAACgQmqRcZwAVPoCAAAAAFSIpC8AAAAAQIUotgYASlOv17V3AACAqpFxLN1hvwW33Xbbm3UOAKAkX/rSl8o+AvAmWlerlX0EAOAYu1VhxXFPewcAAAAAgApRbA0AlEp7BwAAqJBakkllHwKVvgAAAAAAFSLpCwAAAABQIZK+AAAAAAAVoqcvAFAqPX0BAKBCapFxnABU+gIAAAAAVIikLwAAAABAhSi2BgBKpb0DAABUiPYOE4JKXwAAAACACpH0BQAAAACoEMXWAECptHcAAICKkXEsnUpfAAAAAIAKkfQFAAAAAKgQxdYAQGnq9br2DgAAUCW1JJPKPgQqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUSC0yjhOASl8AAAAAgAqR9AUAAAAAqBBJXwAAAACACtFhAwAolZ6+AABQIXr6TggqfQEAAAAAKkTSFwAAAACgQhRbAwCl0t4BAAAqZlLZB0ClLwAAAABAhUj6AgAAAABUiPYOAECptHcAAIAKqUXGcQJQ6QsAAAAAUCGSvgAAAAAAFaLYGgAoTb1e194BAACqRHuHCUGlLwAAAABAhUj6AgAAAABUiGJrAKBU2jsAAECFaO8wIaj0BQAAAACoEElfAAAAAIAKkfQFAAAAAKgQHTYAgFLp6QsAABUzqewDoNIXAAAAAKBCJH0BAAAAACpEewcAoFTaOwAAQIXUIuM4Aaj0BQAAAACoEElfAAAAAIAKUWwNAJRKewcAAKgQ7R0mBJW+AAAAAAAVIukLAAAAAFAhkr4AQGnq9fqEf7Ri27Ztueiii9Le3p7169c3XD9w4ECuu+66tLe35z3veU+eeeaZJMkzzzyTk08+OZ2dnens7Mw//+f/fHTPn//5n+eSSy5Je3t7PvvZz2qDAQDAW8ekCf44Dkj6AgCMw8jISD7zmc/kwQcfzFNPPZV77703Tz311Jg1GzduzJlnnpmf/vSn+fznP58vfOELo9cuvPDC9Pf3p7+/P3feeedo/FOf+lT+03/6TxkYGMjAwEC2bdv2pr0nAADgrU3SFwBgHHp7e9Pe3p4LLrggJ554Yq6//vps3bp1zJqtW7dm1apVSZJrrrkmDz/88GErdwcHB7N///4sWbIktVotK1euzAMPPHAs3wYAAFAhkr4AQKnKbt9wpMfu3bvT1dU1+rjrrrvGnH/nzp0555xzRp/PmzcvO3fubLpm8uTJOf3007Nnz54kydNPP51/9I/+Uf7xP/7H+eEPfzi6ft68eYe9JwAATEi1JJMn+OM4cJy8TQCAX83MmTPT19d3TO49Z86c/PznP8+MGTPy53/+57nqqqvy5JNPHpPXAgAAjh8qfQEAxqGtrS3PPvvs6PMdO3akra2t6Zrh4eG89NJLmTFjRqZOnZoZM2YkSRYtWpQLL7wwf/VXf5W2trbs2LHjsPcEAABoRtIXAChV2e0bjvQ4ksWLF2dgYCBPP/10Dh48mM2bN2f58uVj1ixfvjw9PT1Jkvvvvz8f/OAHU6vVsnv37oyMjCRJ/vqv/zoDAwO54IILMmfOnJx22mn5sz/7s9Tr9dx999258sor3/gvPgAAvNEq3t5h79696e7uTkdHR7q7u7Nv376GNf39/Xnve9+bBQsWZOHChbnvvvtGr33iE5/I+eefn87OznR2dqa/vz9J8tWvfnU0dvHFF2fSpEnZu3dvkuS8887LJZdcks7OznR1dbV0TklfAIBxmDx5cu64444sW7Ys73jHO3LttddmwYIFWbt2bb7zne8kST75yU9mz549aW9vz7/7d/8u69evT5I8/vjjWbhwYTo7O3PNNdfkzjvvzPTp05Mkf/AHf5Bbbrkl7e3tufDCC3P55ZeX9h4BAIDXrV+/PkuXLs3AwECWLl06+rv93zdt2rTcfffdefLJJ7Nt27Z87nOfy4svvjh6/atf/Wr6+/vT39+fzs7OJMnv/u7vjsa+8pWv5B//4388+tkgSR555JH09/e33HpOT18AgHH6yEc+ko985CNjYl/+8pdH/33SSSflj/7ojxr2ffSjH81HP/rRwnt2dXXlf//v//3GHhQAABiXrVu35tFHH02SrFq1Kpdeemluv/32MWvmz58/+u+5c+dm1qxZ2b17d84444yWXuPee+/NDTfcMK5zqvQFAAAAAI4bu3fvTldX1+jjrrvuannv0NBQ5syZkySZPXt2hoaGDru+t7c3Bw8ezIUXXjgau/XWW7Nw4cJ8/vOfz4EDB8asf/XVV7Nt27YxxSG1Wi0f+tCHsmjRopbPqtIXAChVK31zAQCAt4hakkllH+LwZs6cedg2CZdddlmef/75hvi6devGPK/VaqnVak3vMzg4mBtvvDE9PT054YTXa2+/8pWvZPbs2Tl48GBWr16d22+/PWvXrh3d893vfje/8Ru/Maa1w49+9KO0tbVl165d6e7uztvf/vZ84AMfOOx7lPQFAAAAAPj/PPTQQ02vnX322RkcHMycOXMyODiYWbNmFa7bv39/rrjiiqxbty5LliwZjf9dlfDUqVNz00035fd///fH7Nu8eXNDa4e2trYkyaxZs7JixYr09vYeMemrvQMAAAAAQAuWL1+enp6eJElPT0+uvPLKhjUHDx7MihUrsnLlylxzzTVjrg0ODiZ5/S8eH3jggVx88cWj11566aU89thjY+75yiuv5OWXXx799/bt28fsaUalLwBQKu0dAACgQmqpdMZxzZo1ufbaa7Nx48ace+652bJlS5Kkr68vd955ZzZs2JAtW7bk8ccfz549e7Jp06YkyaZNm9LZ2ZmPf/zj2b17d+r1ejo7O3PnnXeO3vvb3/52PvShD+WUU04ZjQ0NDWXFihVJkuHh4XzsYx/Lhz/84SOes8LfAgAAAACAN86MGTPy8MMPN8S7urqyYcOGJMlv//Zv57d/+7cL9//gBz9oeu9PfOIT+cQnPjEmdsEFF+THP/7xUZ9TewcAAAAAgApR6QsAlKZer2vvAAAAVSPjWDqVvgAAAAAAFSLpCwAAAABQIYqtAYBSae8AAAAVUouM4wSg0hcAAAAAoEIkfQEAAAAAKkSxNQBQKu0dAACgQmpJJpV9CFT6AgAAAABUiKQvAAAAAECFSPoCAAAAAFSInr4AQKn09AUAgAqpRcZxAlDpCwAAAABQIZK+AAAAAAAVotgaACiV9g4AAFAxMo6lU+kLAAAAAFAhkr4AAAAAABWi2BoAKE29XtfeAQAAqqSWZFLZh0ClLwAAAABAhUj6AgAAAABUiPYOAECptHcAAIAKqUXGcQJQ6QsAAAAAUCGSvgAAAAAAFaLYGgAolfYOAABQIdo7TAgqfQEAAAAAKkTSFwAAAACgQhRbAwCl0t4BAAAqZlLZB0ClLwAAAABAhUj6AgAAAABUiKQvAAAAAECF6OkLAJRKT18AAKiQWmQcJwCVvgAAAAAAFSLpCwAAAABQIYqtAYDS1Ot17R0AAKBKtHeYEFT6AgAAAABUiKQvAAAAAECFKLYGAEqlvQMAAFSI9g4TgkpfAAAAAIAKkfQFAAAAAKgQxdYAQKm0dwAAgIqZVPYBUOkLAAAAAFAhkr4AAAAAABWivQMAUCrtHQAAoEJqkXGcAFT6AgAAAABUiKQvAAAAAECFSPoCAAAAAFSIDhsAQKn09AUAgArR03dCUOkLAAAAAFAhkr4AAAAAABWi2BoAKE29XtfeAQAAqmZS2QdApS8AAAAAQIVI+gIAAAAAVIj2DgBAqbR3AACACqlFxnECUOkLAAAAAFAhkr4AAAAAABWi2BoAKJX2DgAAUCHaO0wIKn0BAAAAACpE0hcAAAAAoEIUWwMApdLeAQAAKkR7hwlBpS8AAAAAQIVI+gIAAAAAVIikLwAAAABAheiwAQCUSk9fAAColvqksk+ASl8AAAAAgAqR9AUAAAAAqBDtHQCA0tTrde0dAACgQuq1ZETGsXQqfQEAAAAAKkTSFwAAAACgQhRbAwCl0t4BAAAqRHuHCUGlLwAAAABAhUj6AgAAAABUiGJrAKBU2jsAAEB11GvJ8CR1pmXzHQAAAAAAqBBJXwAAAACACtHeAQAolfYOAABQHfVaLSOTpRzLptIXAAAAAKBCJH0BAAAAACpErTUAUCrtHQAAoFpGJk0q+wjHPZW+AAAAAAAVIukLAAAAAFAhkr4AAAAAABWipy8AUJp6va6nLwAAVEg9tYxET9+yqfQFAAAAAKgQSV8AAAAAgArR3gEAKJX2DgAAUB311DKsvUPpVPoCAAAAAFSISl8AAADeMEfzIXPKMdhfZCKeqdl9W93fbF2rZz0W+8ebYHitSXy4IHaoxXWvHoN7NltbdP6j2d9sLcCvQqUvAFCqer0+oR+t2LZtWy666KK0t7dn/fr1DdcPHDiQ6667Lu3t7XnPe96TZ555Jkny/e9/P4sWLcoll1ySRYsW5Qc/+MHonksvvTQXXXRROjs709nZmV27dr0hX28AADjWRjJ5Qj/GY+/evenu7k5HR0e6u7uzb9++hjX9/f1573vfmwULFmThwoW57777Rq/V6/XceuutmT9/ft7xjnfk61//+mj8s5/9bNrb27Nw4cL8z//5P0f39PT0pKOjIx0dHenp6WnpnJK+AADjMDIyks985jN58MEH89RTT+Xee+/NU089NWbNxo0bc+aZZ+anP/1pPv/5z+cLX/hCkuSss87Kd7/73fzlX/5lenp6cuONN47Zd88996S/vz/9/f2ZNWvWm/aeAACAYuvXr8/SpUszMDCQpUuXFhZ9TJs2LXfffXeefPLJbNu2LZ/73Ofy4osvJkk2bdqUZ599Nj/5yU/yf/7P/8n111+fJHnwwQczMDCQgYGB3HXXXfnUpz6V5PUk82233ZYnnngivb29ue222woTzf+QpC8AwDj09vamvb09F1xwQU488cRcf/312bp165g1W7duzapVq5Ik11xzTR5++OHU6/X8o3/0jzJ37twkyYIFC/Laa6/lwIEDb/p7AAAAWvP3f7dftWpVHnjggYY18+fPT0dHR5Jk7ty5mTVrVnbv3p0k+cY3vpG1a9fmhBNeT8v+XXHH1q1bs3LlytRqtSxZsiQvvvhiBgcH873vfS/d3d2ZPn16zjzzzHR3d2fbtm1HPKekLwBQqrLbNxzpsXv37nR1dY0+7rrrrjHn37lzZ84555zR5/PmzcvOnTubrpk8eXJOP/307NmzZ8yaP/7jP8673/3uTJ06dTR20003pbOzM//23/7blltNAABAmeqpZSSTJvTjSL/jH87Q0FDmzJmTJJk9e3aGhoYOu763tzcHDx7MhRdemCT52c9+lvvuuy9dXV25/PLLMzAwkKT554pWPm8UMcgNAOAwZs6cmb6+vmP6Gk8++WS+8IUvZPv27aOxe+65J21tbXn55Zfz0Y9+NN/85jezcuXKY3oOgKN1NIPIWl178jj3F62bdgzu2Wxt0fmb7S9aW3TPwteZWhBMMrngxYrW1podqui+RWtbXdcsXjS17JUm+wv+COZQwdr9f9sYe22k+Jb7C2IvF8SaDZcr2l8Ua7a/6LUMcoM3z5F+x7/sssvy/PPPN8TXrVs35nmtVkutVmt6n8HBwdx4443p6ekZrew9cOBATjrppPT19eVb3/pWbr755vzwhz/8Fd9Jc5K+AADj0NbWlmeffXb0+Y4dO9LW1la4Zt68eRkeHs5LL72UGTNmjK5fsWJF7r777tH/9//v9iTJqaeemo997GPp7e2V9AUAgDfBQw891PTa2WefncHBwcyZMyeDg4NNZ2/s378/V1xxRdatW5clS5aMxufNm5err746SbJixYrcdNNNSZp/rmhra8ujjz46Jn7ppZce8T1o7wAAlKbs1g2tPI5k8eLFGRgYyNNPP52DBw9m8+bNWb58+Zg1y5cvH52ye//99+eDH/xgarVaXnzxxVxxxRVZv359fuM3fmN0/fDwcF544YUkyaFDh/Inf/Inufjii9/ArzwAABwbb4X2DuPx93+37+npyZVXXtmw5uDBg1mxYkVWrlyZa665Zsy1q666Ko888kiS5LHHHsv8+fNH73v33XenXq/nz/7sz3L66adnzpw5WbZsWbZv3559+/Zl37592b59e5YtW3bEc6r0BQAYh8mTJ+eOO+7IsmXLMjIykptvvjkLFizI2rVr09XVleXLl+eTn/xkbrzxxrS3t2f69OnZvHlzkuSOO+7IT3/603z5y1/Ol7/85STJ9u3bc8opp2TZsmU5dOhQRkZGctlll+V3fud3ynybAABAkjVr1uTaa6/Nxo0bc+6552bLli1Jkr6+vtx5553ZsGFDtmzZkscffzx79uzJpk2bkiSbNm1KZ2dn1qxZk49//OP52te+lre97W3ZsGFDkuQjH/lI/vRP/zTt7e2ZNm1a/vAP/zBJMn369Hzxi1/M4sWLkyRr167N9OnTj3jOWv0wJSy33XbbuL4IAMDE96Uvfam01z7vvPNKff1W/If/8B+OeU9feDOtO0zfOThaevo20tO3xbievi2vhV/FrSUOAb6k68R8p++s0l6/Ff+sa27lf8fX3gEAAAAAoEK0dwAAStVK31wA3jqOpqq11VhSXK1b9FpHc89W1zarFC6qoD35pMbYlCZVuYXVsgX7c0qLe5vtL1rbbP/bxrG/6LWT4vMXVfW+1GR/QXxKQWxGUantUdxzf0FsT0GVcZLsLYgV/ewUrUtU9VJ94+2by/ip9AUAAAAAqBBJXwAAAACACtHeAQAolfYOAABQHfXUMqy9Q+lU+gIAAAAAVIhKXwAAAH4lU1qMJa0PTTvtKPYXrT2qexYMIzu1YOhY7WgGlLUaG+/+ZvcsGsRW9plaHeS2q8n+PS2uLZqaNtTkngVrTyu456lFr51kSrMJbf9As4FtRT+nL7d2S4CWSPoCAKXS3gEAAKqjnlpGpBxLp70DAAAAAECFSPoCAAAAAFSIWmsAoFTaOwAAQLWMZFLZRzjuSfoCAAAcpw6Nc3/RMKpmHzKLBrwVDVhrNnTt1Fb3tzicLUlqpxcEj8XQs6LXaRZvNdbsTEVri75Q4z1Tweu/cnrxHxO/PLXxu3fqgcaxZac898viMz03jtj04lsWri0a4lfw85QkbQWxQwXD3ZoNZ2txDhzAr0x7BwAAAACAClHpCwCUpl6va+8AAAAVUk9Ne4cJQKUvAAAAAECFSPoCAAAAAFSI9g4AQKm0dwAAgOqoJxnW3qF0kr4AAAAVc+hNep2iD5RTmqw9ucXYqU32zyhae0pjbNrpBQtPa3LTorWtxpKk4PUL105vsr/oTbW6v9mZiu5ZsP+V04v/8PfFqWc0xF4u+K68mNbWNVt7xtQXG2Jzz3+ucP+cgvj0p3/RuPDnBZuLvh5J8ffupCZrC9QOFLzUK42xXQXrEskY4NjT3gEAAAAAoEIkfQEAAAAAKsRfFAAApdLTFwAAqqSWESnH0qn0BQAAAACoEGl3AACAt6g3a2Db0Wj2IbPVQW7NZq4VDm1rdcBZs2FeRS/W6nC1pPUBa7Oa7G9x6FrObgz9oslwuD2nNF4oGqS2ryCWJHtyVkv7i2IvNPlCFw5yy4sNsfPzTOH+8wri553/dEPsnOnPN8SmFA1sS5JJTeL/UMG8uCRJwdC2015qjJ26q3h7s4GHAG8USV8AoFTaOwAAQHXUU8tIy//PCseK9g4AAAAAABUi6QsAAAAAUCHaOwAApdLeAQAAqmXit3cYKfsAx5ykLwAAwFvARBzaVqRoOFuzeNEctdOmFu+fVjQgrdWhbU2GnhUOWDua/eMYxJYkmdsYemVW4x/kvjC18YWKBq69Hm9cWzRgbVeTQxWtLXqtVl+n2f4ZeWFcZyoaDvfa6f+3IXbhO/668J4nFQWLckAFA9uSJAVD24p+Hk9rMsit2X8nAG8U7R0AAAAAACpEpS8AUJp6va69AwAAVEg9Ne0dJgCVvgAAAAAAFSLpCwAAAABQIdo7AACl0t4BoNFbZWjblBZjzeJFw6xOPaXJDYqmvrU6SK1oYFtSPGCtaG2zQW4t7q8XDGxLkqHpjZO/hgpuUDTgbKjJ0LNdBfuL1ha9TvPXavFMe4rPdGhH4zdvyrz9DbE9M4qH072cUxtir2VaQ6zwz8mb/DzN72gc8DalaGjbnuL92VsQG2wMndzkL9ynVP8vyzmO1VPL8IRv71B9Kn0BAAAAACpE0hcAAAAAoEIkfQEAAAAAKkRPXwCgVHr6AgBAtYxIOZZOpS8AAAAAQIVIuwMAAPCGmdIkfnJB7LSpjbHaKU1u8LaiGxTEZhTEzm5yz1kFsbktrmsS/0XB/udOmV24/bmCF9tVcNiidUWx1+NzfuV7Novv/pvGe2ZHwXf6mcJbFsYPndf4zfvxkvcUbn/1wsafnoM5scmLjTUpI4Xxk09/tSF2wa8937hwV5MbP1cQK/h5PK3o5zbJ5Jea3BfgDSLpCwCUSnsHAACojnpqGcmkso9x3NPeAQAAAACgQiR9AQAAAAAqRHsHAKBU2jsAAEB1aO8wMUj6AgAAlOhQ2QcYh6IPlM0+ZBYNcjv5pIJgs0FuRUPbTi+ITW8xlhQPaCuK/Vrx9v1zG4eZDU1qHJr2bM4p3F80NK1obdFwtmb3HCy4588L1j6/s3iQW35a8E15pmhdQaxoXbO17QWxv60Vbh8Yfldj8KLG0Ik52BA7OY0D25LkjLzYGDu/MTb9uV8U7i/8OSn4eZzS5Od5ikFuwDGmvQMAAAAAQIWo9AUASlOv17V3AACAihnW3qF0Kn0BAAAAACpE0hcAAAAAoEK0dwAASqW9A0C1NI42e13RILcpUwuCb2tyg6KhbTNajBUN3UpSMPOsMFY0sC1JnpvUuLjV4WxJ8kzOa2ltUezpgr1J8uxLjWt/8dOCSXY/KdxePHSt1Vize75YMK7wJwVf0yYz04oMTF7YEJt24WsNsVPzcuH+s7KnIXZ2hhpi0+f+dfEBin7Oin5Gi37GIxlDtdVTy4if8tKp9AUAAAAAqBBJXwAAAACACpH0BQAAAACoEA02AIBS6ekLAADV8XpP30llH+O4p9IXAAAAAKBCVPoCAAC8CQ6VfYBjYEpBrNmHzKK1OaXFWJKc3mJsekHs7Cb3nNUYemVuY23Uc5PmFm5/Lo3xZ3NOQ+yZnFe4/+mC+DM5v6X9f7Oz+J753yc1xn5SsK4odjRrny/6S52/bHLTv2gMvbiwMfZQQSxJCt5S3lZrCD11xjsbYjNmvFB4y1kZaoidk2cbYufNfaZw/ymzftkYLPp5bPLzfHJxGOANI+kLAJRKewcAAKgW7R3Kp70DAAAAAECFSPoCAAAAAFSI9g4AQKm0dwAAgOqop5Zh7R1KJ+kLAADAG6ZwYFuSk6cWBIs+kRataxYvGpJ1FMO0ita+OPWMxlgaY83iezKjIfZCQez1tWc1xHYVTJcbeqlg4tyOoulmSZ4viO1oMdYsXnTP/LQgNtDkpkVrC0aZFQ13O4ozHXrhtMZbzjij8JZ/m1MbYi8XxF6dOq1w/ykn/W1jsNWfcYA3gfYOAAAAAAAV4v9zAgBKU6/XtXcAAIAKqaeWESnH0qn0BQAAAACoEElfAAAAAIAKUWsNAJRKeweA48Oh4XHeYKQgVnTPAy2uaxI/MQdbiiXJpIJDnZxXG2KnpmDoV5Iz8mJDbFZ2NcQOnN44IezZ9uKP84fSOMys8JP/2wq3p3BmXeO8ueSnHY2xF9qb3PQdBbF3NoaWNNl+aUGsYO25F/2kIdaenxXe8rw80xCbm+caYjN3FX/vsqcg9lJB7BfF28f7nwNMdCOZVPYRjnsqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABURz017R0mAJW+AAAAAAAVIukLAAAAAFAh2jsAAADwhjl0NIuHW4wdzdqjueeBxtCkgsVTixY2iU/NwYbYtLxauP/UvNwQOysvNMQO5sSG2MiM4j+dHprceP5fnDS9ceHbCrcnZxTEZhfE5hXEnqkV33PHOxtj7QXrLm1ypt9sDE1fsrMhdlH+b0Pswvys8Jbn5ZmG2Dl5tnHhc03OtKsgtr8g9krx9qP67wTgVyDpCwCUSk9fAACoDj19JwbtHQAAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUy7D2DqWT9AUAAHgTTGkSN9DpHxgZ59qioW3N7lmwdvJI4+JJk4pvcGLB0LaiWNHAtiQ5Iy82xIr6YBYlTyY1eVMnnt44XG7Pxa81xPaeNaNwf2af1Bh7vmDdeQWxonVHs79gYFuSTP/NxqFtl0z6y4bYO/NUQ6xouFtSPLRt7t69jQubDXLbUxB7qTFUbzLIrdlsQYA3ivYOAAAAAAAVotIXAChNvV7X3gEAACqknlpGpBxLp9IXAAAAAKBCJH0BAAAAACpErTUAUCrtHQCOD8OtDl1reoMWY0fzOgVrT/xF42i9E09pHI6WFA9Tm5rGtdPyauH+M7KvycGO/DrT0jicrdlrnTnpxYbYvrYXCve/2HZmY2zPGQ2xQy+c1ri5+JbF8fMaQ7Pf9deF24sGtLUauzA/LbzneXm6IVb7ecHCZoPcdhXECga5vVb8o9PkuwfV8Hp7h8YBlLy5VPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqBbtHcqn0hcAAAAAoEJU+gIAADBxHMXQtZaHu/2i9deaVBQrfPHioW0n5mBD7OSjGNs1eZzD4U7Nyw2xVzOtIfZy3la4/+Wc2hD72xmNsZcLYq9e1Pg6SfLySOPasycNNcTm5/8W7r8of1UQa1x7YX7WEGsviCXJ9IGCH4rG2W7NB7ntLYgVDHJ7+ZXi7Y3jAgHeWCp9AQAAAABasHfv3nR3d6ejoyPd3d3Zt29fw5r+/v68973vzYIFC7Jw4cLcd999o9fq9XpuvfXWzJ8/P+94xzvy9a9/PUlyzz33ZOHChbnkkkvyvve9Lz/+8Y9H95x33nm55JJL0tnZma6urpbOqdIXACiVnr4AAFAd9dQyXOGevuvXr8/SpUuzZs2arF+/PuvXr8/tt98+Zs20adNy9913p6OjI88991wWLVqUZcuW5YwzzsimTZvy7LPP5ic/+UlOOOGE7Nq1K0ly/vnn57HHHsuZZ56ZBx98MKtXr84TTzwxes9HHnkkZ511VsvnlPQFAAAAAGjB1q1b8+ijjyZJVq1alUsvvbQh6Tt//vzRf8+dOzezZs3K7t27c8YZZ+Qb3/hG/vN//s854YTXGzDMmjUrSfK+971vdM+SJUuyY8eOcZ1TewcAAAAA4Lixe/fudHV1jT7uuuuulvcODQ1lzpw5SZLZs2dnaKixR/nf19vbm4MHD+bCCy9MkvzsZz/Lfffdl66urlx++eUZGBho2LNx48Zcfvnlo89rtVo+9KEPZdGiRS2fVaUvAFAq7R0AAKA66qllZIKnHGfOnJm+vr6m1y+77LI8//zzDfF169aNeV6r1VKr1ZreZ3BwMDfeeGN6enpGK3sPHDiQk046KX19ffnWt76Vm2++OT/84Q9H9zzyyCPZuHFjfvSjH43GfvSjH6WtrS27du1Kd3d33v72t+cDH/jAYd/jxP4OAAAAUF0jBbHhJmuL4q3uL1qXJL9oDE050BibmoOF208siE9NwQ2amFRw2KJ7npxXG2Jn5MXCe76aaQ2xgzmxpXVHs/ZAwbrXmtzzwKTGtWdlT0PswvyscP95eboh1l6w9rw80xCb+fTfFt4zjYV1yc8LYs8Vb09RYd9LjaH9TbYfahIHJoaHHnqo6bWzzz47g4ODmTNnTgYHB0fbM/xD+/fvzxVXXJF169ZlyZIlo/F58+bl6quvTpKsWLEiN9100+i1v/iLv8gtt9ySBx98MDNmzBiNt7W1JXm9FcSKFSvS29t7xKSv9g4AAAAAAC1Yvnx5enp6kiQ9PT258sorG9YcPHgwK1asyMqVK3PNNdeMuXbVVVflkUceSZI89thjo/1/f/7zn+fqq6/ON7/5zTE9gV955ZW8/PLLo//evn17Lr744iOeU9IXAChNvV6f8I9WbNu2LRdddFHa29uzfv36husHDhzIddddl/b29rznPe/JM888M3rtK1/5Strb23PRRRfle9/7Xsv3BACAiWokkyb0YzzWrFmT73//++no6MhDDz2UNWvWJEn6+vpyyy23JEm2bNmSxx9/PJs2bUpnZ2c6OzvT398/uv+P//iPc8kll+Tf/Jt/kw0bNiRJvvzlL2fPnj359Kc/nc7OznR1dSV5vYfwb/7mb+Zd73pXfv3Xfz1XXHFFPvzhDx/xnNo7AACMw8jISD7zmc/k+9//fubNm5fFixdn+fLleec73zm6ZuPGjTnzzDPz05/+NJs3b84XvvCF3HfffXnqqaeyefPmPPnkk3nuuedy2WWX5a/+6q+S5Ij3BAAA3nwzZszIww8/3BDv6uoaTeD+9m//dn77t3+7cP8ZZ5yR//Jf/ktDfMOGDaP7/74LLrggP/7xj4/6nCp9AQDGobe3N+3t7bngggty4okn5vrrr8/WrVvHrNm6dWtWrVqVJLnmmmvy8MMPp16vZ+vWrbn++uszderUnH/++Wlvb09vb29L9wQAAGhGpS8AUKpWWyiUZffu3aN/WpUkq1evzurVq0ef79y5M+ecc87o83nz5uWJJ54Yc4+/v2by5Mk5/fTTs2fPnuzcubNhqMPOnTuT5Ij3BKpjSkHsrTzkqdkctmPynoperNVY0vIguElNJsEVDW07Oa+Na/9wwZ8dn1Hw0b3ZnycXDVgbOYr9RfFjcc8ZBYPczsmzhfuL4ue8sqMhdlLjvLfkrwtvmYLZcMWxZoPc9jaGDhVMbWv8aXhdsx9JqIJ6auNuocD4SfoCABzGzJkz09fXV/YxAAAAWqa9AwDAOLS1teXZZ/9fBdKOHTvS1tbWdM3w8HBeeumlzJgxo+neVu4JAADQjKQvAFCqer0+oR9Hsnjx4gwMDOTpp5/OwYMHs3nz5ixfvnzMmuXLl6enpydJcv/99+eDH/xgarVali9fns2bN+fAgQN5+umnMzAwkF//9V9v6Z4AADBRjWTShH4cD7R3AAAYh8mTJ+eOO+7IsmXLMjIykptvvjkLFizI2rVr09XVleXLl+eTn/xkbrzxxrS3t2f69OnZvHlzkmTBggW59tpr8853vjOTJ0/Of/gP/yGTJr3+S2jRPQEAAFoh6QsAME4f+chH8pGPfGRM7Mtf/vLov0866aT80R/9UeHeW2+9NbfeemtL9wSYaI7JcLbimWetT75qcThb03jjbLVMHSkIJpk0qfHFioazFcUmqmZD595oZ+TFhtjcA8VT00557peNwZ8XLCzaXjwbrnht0T13NdlfEB96qTFWMNstSfMBbwBvFElfAKBUrbRQAAAA3hrqqWX4OGmhMJHp6QsAAAAAUCGSvgAAAAAAFSLpCwAAAABQIXr6AgCl0tMXAACqo55aRqQcS+c7AAAAQDmGx7m21dhIk3seaG3/ib84VLj9xFMab3BiDjbEJjV5o1ML1zYe9sSCg05u8qZa3j/SZP9wwf5f/LJgf+Pe2i8Kb1n89d9TENvVZH9RfKjFdUdzz1ZfJ8megvjegnUvN3n54p8ogDeO9g4AAAAAABWi0hcAKE29XtfeAQAAKmYkk8o+wnFPpS8AAAAAQIVI+gIAAAAAVIj2DgBAqbR3ADg+tDy4qtlwt6JhYOMZ7tYsXjDcbVKT/UWD2KYW3ODkvFq4f1pea4gVDV2b9krjhLSTXik+U+FwuqK1fzvO/UWxZoPcita+VBArmoR2NGuLhsMV7W2yv16wf2+T/UMFP49FRyr+zh/dDEN4q6mnpr3DBKDSFwAAAACgQiR9AQAAAAAqRHsHAKBU2jsAAEB11FPLsPYOpVPpCwAAAABQISp9AQAAJpgpBbGWB6G9hdQLplnVjmbCVdFwt6JY0XCypOWhb1Oa7D+xYJBbUaxoYFuSnHrg5YbYKS/9snFh0TCxZgPK9re4ttn+Vte2+jrN4q0Od2u2tsXYq03u+XLB2qK3VBRLioe2Fa1t/A6/ror/PQMTi6QvAFAq7R0AAKBaRqQcS6e9AwAAAABAhUj6AgAAAABUiKQvAAAAAECFaLABAJRKT1+Aamk2oOpo5rO1fIPxxJLiAW9HsX9SwYVpebUhVjSwLUlO2VMwtK1oQtiugtie4jMV7i9aW3TPZmvHeaZDBfv3/21j7LWiIXwp/pkqGo3XaqxZvOjb3PjdPHavD1VRTy0jmVT2MY57Kn0BAAAAACpE0hcAAAAAoEK0dwAASlOv17V3AACACtHeYWJQ6QsAAAAAUCGSvgAAAAAAFaK9AwBQKu0dAFozpSB26E0/xRtreKQxNmW42eI3ONYsfqDFWJKpBw42xEamNn7MnvbKL4tvsLcgtqsg9lyLsaPZP9Rkf8HaesE9nyk4+980ueWOgthrBbFm36ZWjfe/h6N5/TfzteCtSHuH8qn0BQAAAACoEElfAAAAAIAK0d4BACiV9g4AAFAd9dQyrL1D6VT6AgAAAABUiEpfAAAA3jDNBlQVDb46VLB4SpOhaYXD1F4ZRyxJXiqInVIQm1q8/ZSpjQPaTjzlbxtitaLhakmypyBWNGCtKFa092jWNjtTwYC2nQWxou3NZsPtLIgZZAZwbEn6AgCl0t4BAACqo55aRqQcS6e9AwAAAABAhUj6AgAAAABUiKQvAAAAAECFaLABAJRKT1+AX92UJvGioWnHwtEM4ypa+9ovGmPTmg1ya3VAW9FwtpOa3LPoE/E4PyVPeVtBsNmEs6JpaAVD07K/IFb0PpPWv05Nvs6vFqx9rWBdUezN+rkDJr6RTCr7CMc9lb4AAAAAABUi6QsAAAAAUCHaOwAApanX69o7AABAhdRT095hAlDpCwAAAABQIZK+AAAAAAAVor0DAFAq7R0A3nhTCmKH3qTXbvY6rxXFRgr2v1K8f0pR/KWC2NSC2ElNDlX018fj/Yvk/QWxPU3W7mpxbVGs6HWS4q9JUazJ1/m1XzTGir6nb9bPE/DWU08tw9o7lE6lLwAAAABAhUj6AgAAAABUiPYOAECptHcAAIBqGZFyLJ1KXwAAAACACpF2BwAAOA68WcPdmt2zKF443K1gkFjSZJBb0dC2ogFn4/3kWzBwLkkyXBArGhrXbJDb3hbXtrouKX7/BV+7ZgPziobrFX2fit56UQyAckj6AgCl0t4BAACqo55aRjKp7GMc97R3AAAAAACoEElfAAAAAIAK0d4BACiV9g4AAFAd2jtMDJK+AAAAx6mi4W5J6wPeitY1G+ZVOLStILb/QPH+kwsGj00pGuRW9Cl3vLmHoxnkVnSmokFsSfJSi2uL1hXFkuRvC2IFX7tmA/NaHdpW9L0/FoMBAfjVaO8AAAAAAFAhkr4AAAAAABWivQMAUJp6va6nLwAAVMywnr6lU+kLAAAAAFAhKn0BAAAYo2jA23iGuyXFw8BaHe6WJPsLBpRNLygkqzXZX6hoQFvRQZsMPSuMn1QQazZ0rdUBbUczyG1/Y+jVgkFuzQbmtfo9MbQNYGKT9AUASqW9AwAAVEc9tYxIOZZOewcAAAAAgAqR9AUAAAAAqBC11gBAqbR3AACA6ni9vUNB03XeVCp9AQAAAAAqRKUvAAAARzSlxXXDTeKHCmKvtRhLkv0jjbEprzTGTi3YW2tyz8LDFsUONNlfFC/6lP23TfbvL4gVvKe81OK6JK8WrH25YG3RSyetf09a/dIBUA5JXwCgVNo7AABAtWjvUD7tHQAAAAAAKkTSFwAAAACgQrR3AABKpb0DAABURz21Cd/eYWKf7o0h6QsAAMCvpGhwV9HAtqR4GFjRcLhmA8YKFQxSO1RwqNMKhsAlyZSiQWxFsVOavH7RMLWiT9lNhq4VDngrev2C/a82uWerQ9uaDcwrihd9T5t9nwGYGLR3AAAAAACoEElfAAAAAIAK0d4BAChNvV7X0xcAACqknmR4gnfNndine2Oo9AUAAAAAqBCVvgAAALxhioa7JcWDv14tiI33Q+pwwdC24ZeK1578i8bYqQWxWrNBbEUD3orKx5rtLxjaVjSg7bWCM+1vMpyuaBDbyy2uS1of+tbqwLek+c8EAMeOpC8AUCrtHQAAoEpqGZFyLJ32DgAAAAAAFSLpCwAAAABQIWqtAYBSae8AAADVUU8tI4UNznkzqfQFAAAAAKgQlb4AAAD8Sg4VxJp9yHxtnK813GLs5KN47ZMPNMb2F8ROblKwdvJJjbEpBV+A135RvP+1gtd6tWhdi7GjWTve/a1+PwCqZu/evbnuuuvyzDPP5LzzzsuWLVty5plnjlnT39+fT33qU9m/f38mTZqUW2+9Ndddd12S1//S8fd+7/fyR3/0R5k0aVI+9alP5bOf/WweffTRXHnllTn//POTJFdffXXWrl2bJNm2bVv+5b/8lxkZGcktt9ySNWvWHPGckr4AQKm0dwAAgGqpcnuH9evXZ+nSpVmzZk3Wr1+f9evX5/bbbx+zZtq0abn77rvT0dGR5557LosWLcqyZctyxhlnZNOmTXn22Wfzk5/8JCeccEJ27do1uu/9739//uRP/mTMvUZGRvKZz3wm3//+9zNv3rwsXrw4y5cvzzvf+c7DnlN7BwAAAACAFmzdujWrVq1KkqxatSoPPPBAw5r58+eno6MjSTJ37tzMmjUru3fvTpJ84xvfyNq1a3PCCa+nZWfNmnXY1+vt7U17e3suuOCCnHjiibn++uuzdevWI55T0hcAAAAAoAVDQ0OZM2dOkmT27NkZGho67Pre3t4cPHgwF154YZLkZz/7We677750dXXl8ssvz8DAwOja//7f/3ve9a535fLLL8+TTz6ZJNm5c2fOOeec0TXz5s3Lzp07j3hO7R0AgFJp7wAAANVRTy3DE7y9w+7du9PV1TX6fPXq1Vm9evXo88suuyzPP/98w75169aNeV6r1VKr1Zq+zuDgYG688cb09PSMVvYeOHAgJ510Uvr6+vKtb30rN998c374wx/m3e9+d/7mb/4mb3vb2/Knf/qnueqqq8YkhI+WpC8AAABvmGbDvIriRzPcrWh/0SC5onsWDXdLij8QF62dMlK8/+RXmtz4Hyg6Z9L6gLSidUdzz6K1zb5Pre5v9voAbwUzZ85MX19f0+sPPfRQ02tnn312BgcHM2fOnAwODjZtz7B///5cccUVWbduXZYsWTIanzdvXq6++uokyYoVK3LTTTclSU477bTRNR/5yEfy6U9/Oi+88ELa2try7LPPjl7bsWNH2trajvgetXcAAAAAAGjB8uXL09PTkyTp6enJlVde2bDm4MGDWbFiRVauXJlrrrlmzLWrrroqjzzySJLksccey/z585Mkzz///OhfQfb29uaXv/xlZsyYkcWLF2dgYCBPP/10Dh48mM2bN2f58uVHPKdKXwCgNPV6XXsHAACokHpqGalwynHNmjW59tprs3Hjxpx77rnZsmVLkqSvry933nlnNmzYkC1btuTxxx/Pnj17smnTpiTJpk2b0tnZmTVr1uTjH/94vva1r+Vtb3tbNmzYkCS5//77841vfCOTJ0/OySefnM2bN6dWq2Xy5Mm54447smzZsoyMjOTmm2/OggULjnjOWv0wn7Ruu+22N+BLAQBMZF/60pdKe+2zzjqr8P8Zn0h+/OMfH/ZPv+CtZt1h+s7B0Sr6SD+lydqitgmttldodt/CVgxHcc+W2zs02d/svv9QFds7tHr2ZmuBY+vWEgsrTuy6JGf3fbu012/F2V3XV/53fO0dAAAAAAAqpLq11gAAAEwYrQ7+erlJvOjDa1EF6dFUH7e69mj2H41Wh9MdTaVuq2ubfT9aXXs09wTgzSfpCwCUSk9fAAColpFMKvsIxz3tHQAAAAAAKkTSFwAAAACgQiR9AYBS1ev1Cf0Yj71796a7uzsdHR3p7u7Ovn37Ctf19PSko6MjHR0d6enpSZK8+uqrueKKK/L2t789CxYsyJo1a0bXb9q0KTNnzkxnZ2c6OzuzYcOGcZ0TAADeKPXUMpJJE/pxPJD0BQA4RtavX5+lS5dmYGAgS5cuzfr16xvW7N27N7fddlueeOKJ9Pb25rbbbhtNDv+rf/Wv8pOf/CT/63/9r/zX//pf8+CDD47uu+6669Lf35/+/v7ccsstb9p7AgAAJj6D3AAAjpGtW7fm0UcfTZKsWrUql156aW6//fYxa773ve+lu7s706dPT5J0d3dn27ZtueGGG/JP/sk/SZKceOKJefe7350dO3a8qecHOJLho1h7aJz7i7T6gXbKMbjn0dy36L03M96vSauvNd7v3XjvCcCxJekLAJRqvC0UjrXdu3enq6tr9Pnq1auzevXqlvYODQ1lzpw5SZLZs2dnaGioYc3OnTtzzjnnjD6fN29edu7cOWbNiy++mO9+97v5l//yX47G/viP/ziPP/545s+fn6997Wtj7gEAAGWpp5aRXx4fLRQmMklfAIDDmDlzZvr6+ppev+yyy/L88883xNetWzfmea1WS61WO+rXHx4ezg033JDPfvazueCCC5Ik//Sf/tPccMMNmTp1av7jf/yPWbVqVX7wgx8c9b0BAIBqkvQFABiHhx56qOm1s88+O4ODg5kzZ04GBwcza9ashjVtbW2jLSCSZMeOHbn00ktHn69evTodHR353Oc+NxqbMWPG6L9vueWW/Ot//a/H9R4AAIBqMcgNAChVvV6f0I/xWL58eXp6epIkPT09ufLKKxvWLFu2LNu3b8++ffuyb9++bN++PcuWLUuS/N7v/V5eeuml/Pt//+/H7BkcHBz993e+85284x3vGNc5AQDgDVNPhocnTejH8UClLwDAMbJmzZpce+212bhxY84999xs2bIlSdLX15c777wzGzZsyPTp0/PFL34xixcvTpKsXbs206dPz44dO7Ju3bq8/e1vz7vf/e4kyb/4F/8it9xyS77+9a/nO9/5TiZPnpzp06dn06ZNZb1FgAZHM8zL4C8AODYkfQEAjpEZM2bk4Ycfboh3dXVlw4YNo89vvvnm3HzzzWPWzJs3r2ml8Ve+8pV85StfeWMPCwAAVIakLwBQmjeihQIAADBx1Ou1jAxLOZZNT18AAAAAgAqR9AUAAAAAqBBJXwAAAACACtFgAwAolZ6+AABQHa/39J1U9jGOeyp9AQAAAAAqRNIXAAAAAKBCtHcAAEqlvQMAAFRIPdo7TAAqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUR71ey/Ah7R3KptIXAAAAAKBCJH0BAAAAACpEewcAoFTaOwAAQJXU8ssRKceyqfQFAAAAAKgQSV8AAAAAgApRaw0AlKZer2vvAAAAVVJPMjyp7FMc91T6AgAAAABUiKQvAAAAAECFSPoCAAAAAFSInr4AQKn09AUAgAqp1/T0nQBU+gIAAAAAVIikLwAAAABAhWjvAACUSnsHAACokHqS4VrZpzjuqfQFAAAAAKgQSV8AAAAAgArR3gEAKJX2DgAAUDHDZR8Alb4AAAAAABUi6QsAAAAAUCHaOwAApdLeAQAAKqQe7R0mAJW+AAAAAAAVIukLAAAAAFAh2jsAAKWp1+vaOwAAQJVo7zAhqPQFAAAAAKgQSV8AAAAAgArR3gEAKJX2DgAAUCH1JIfKPgQqfQEAAAAAKkTSFwAAAACgQiR9AQAAAAAqRE9fAKBUevoCAECF1JOMlH0IVPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqJjhsg+ASl8AAAAAgAqR9AUAAAAAqBDtHQCA0tTrde0dAACgSurR3mECUOkLAAAAAFAhkr4AAAAAABWivQMAUCrtHQAAoEK0d5gQVPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqBDtHSYElb4AAAAAABUi6QsAAAAAUCGSvgAAAAAAFaKnLwBQKj19AQCgQvT0nRBU+gIAAAAAVIikLwAAAABAhWjvAACUSnsHAACoGO0dSqfSFwAAAACgQiR9AQAAAAAqRHsHAKA09XpdewcAAKiSepJDZR8Clb4AAAAAABUi6QsAAAAAUCHaOwAApdLeAQAAKqSeZKTsQ6DSFwAAAACgQiR9AQAAAAAqRHsHAKBU2jsAAECF1JMMl30IVPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqBDtHSYElb4AAAAAABUi6QsAAAAAUCGSvgAAAAAALdi7d2+6u7vT0dGR7u7u7Nu3r2FNf39/3vve92bBggVZuHBh7rvvvtFr9Xo9t956a+bPn593vOMd+frXv54k+epXv5rOzs50dnbm4osvzqRJk7J3794kyXnnnZdLLrkknZ2d6erqaumcevoCAKXS0xcAACqk4j19169fn6VLl2bNmjVZv3591q9fn9tvv33MmmnTpuXuu+9OR0dHnnvuuSxatCjLli3LGWeckU2bNuXZZ5/NT37yk5xwwgnZtWtXkuR3f/d387u/+7tJku9+97v52te+lunTp4/e85FHHslZZ53V8jlV+gIAAAAAtGDr1q1ZtWpVkmTVqlV54IEHGtbMnz8/HR0dSZK5c+dm1qxZ2b17d5LkG9/4RtauXZsTTng9LTtr1qyG/ffee29uuOGGcZ1T0hcAAAAAOG7s3r07XV1do4+77rqr5b1DQ0OZM2dOkmT27NkZGho67Pre3t4cPHgwF154YZLkZz/7We677750dXXl8ssvz8DAwJj1r776arZt25aPfvSjo7FarZYPfehDWbRoUctn1d4BAChNvV7X3gEAAKpmgrd3mDlzZvr6+ppev+yyy/L88883xNetWzfmea1WS61Wa3qfwcHB3Hjjjenp6Rmt7D1w4EBOOumk9PX15Vvf+lZuvvnm/PCHPxzd893vfje/8Ru/Maa1w49+9KO0tbVl165d6e7uztvf/vZ84AMfOOx7lPQFAAAAAPj/PPTQQ02vnX322RkcHMycOXMyODhY2J4hSfbv358rrrgi69aty5IlS0bj8+bNy9VXX50kWbFiRW666aYx+zZv3tzQ2qGtrS3J660gVqxYkd7e3iMmfbV3AAAAAABowfLly9PT05Mk6enpyZVXXtmw5uDBg1mxYkVWrlyZa665Zsy1q666Ko888kiS5LHHHsv8+fNHr7300kt57LHHxtzzlVdeycsvvzz67+3bt+fiiy8+4jlV+gIApdLeAQAAKqSeCd/eYTzWrFmTa6+9Nhs3bsy5556bLVu2JEn6+vpy5513ZsOGDdmyZUsef/zx7NmzJ5s2bUqSbNq0KZ2dnVmzZk0+/vGP52tf+1re9ra3ZcOGDaP3/va3v50PfehDOeWUU0ZjQ0NDWbFiRZJkeHg4H/vYx/LhD3/4iOes1Q/zSeu22277ld48APDW8aUvfam01z7ttNPynve8p7TXb8W+ffsO2+8L3mrWHabvHABQDbeWWFhR+7Wu5F9N7N+fF93dVfnf8bV3AAAAAACoEO0dAIBSae8AAAAVUvH2Dm8VKn0BAAAAACpE0hcAAAAAoEK0dwAASqW9AwAAVEg9yaGyD4FKXwAAAACACpH0BQAAAACoEElfAAAAAIAK0dMXACiVnr4AAFAh9SQjZR8Clb4AAAAAABUi6QsAAAAAUCGSvgBAaer1+oR/jMfevXvT3d2djo6OdHd3Z9++fYXrenp60tHRkY6OjvT09IzGL7300lx00UXp7OxMZ2dndu3alSQ5cOBArrvuurS3t+c973lPnnnmmXGdEwAA3lDDE/xxHJD0BQA4RtavX5+lS5dmYGAgS5cuzfr16xvW7N27N7fddlueeOKJ9Pb25rbbbhuTHL7nnnvS39+f/v7+zJo1K0mycePGnHnmmfnpT3+az3/+8/nCF77wpr0nAABg4pP0BQA4RrZu3ZpVq1YlSVatWpUHHnigYc33vve9dHd3Z/r06TnzzDPT3d2dbdu2tXzfa665Jg8//LCBeAAAwKjJZR8AADi+TfRk5e7du9PV1TX6fPXq1Vm9enVLe4eGhjJnzpwkyezZszM0NNSwZufOnTnnnHNGn8+bNy87d+4cfX7TTTdl0qRJ+ehHP5rf+73fS61WG7Nn8uTJOf3007Nnz56cddZZv9J7BACAN0w9x00LhYlM0hcA4DBmzpyZvr6+ptcvu+yyPP/88w3xdevWjXleq9VSq9WO6rXvueeetLW15eWXX85HP/rRfPOb38zKlSuP6h4AAMDxR9IXAGAcHnrooabXzj777AwODmbOnDkZHBwc7cn797W1teXRRx8dfb5jx45ceumlo9eS5NRTT83HPvax9Pb2ZuXKlWlra8uzzz6befPmZXh4OC+99FJmzJjxhr4vAADgrUtPXwCgVPV6fUI/xmP58uXp6elJkvT09OTKK69sWLNs2bJs3749+/bty759+7J9+/YsW7Ysw8PDeeGFF5Ikhw4dyp/8yZ/k4osvbrjv/fffnw9+8INHXUUMAADHxN+1d5jIj+OASl8AgGNkzZo1ufbaa7Nx48ace+652bJlS5Kkr68vd955ZzZs2JDp06fni1/8YhYvXpwkWbt2baZPn55XXnkly5Yty6FDhzIyMpLLLrssv/M7v5Mk+eQnP5kbb7wx7e3tmT59ejZv3lzaewQAACYeSV8AgGNkxowZefjhhxviXV1d2bBhw+jzm2++OTfffPOYNaecckr+/M//vPC+J510Uv7oj/7ojT0sAABQGZK+AECpxttCAQAAmEDqSQ6VfQj09AUAAAAAqBBJXwAAAACACpH0BQAAAACoED19AYBS6ekLAAAVUk8yUvYhUOkLAAAAAFAhkr4AAAAAABWivQMAUJp6va69AwBU3KGyDwC8+YbLPgAqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUSD3aO0wAKn0BAAAAACpEpS8AAABwVAxnA5jYJH0BgFJp7wAAABVSj/9naALQ3gEAAAAAoEIkfQEAAAAAKkR7BwCgVNo7AABAhdSTjJR9CFT6AgAAAABUiEpfAAAAoCnzmADeeiR9AYBSae8AAAAVUk8yXPYh0N4BAAAAAKBCJH0BAAAAACpE0hcAAAAAoEL09AUASlOv1/X0BYAJxNA24A2hp2/pVPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqJB69IqZAFT6AgAAAABUiEpfAAAAOM4owgOoNklfAKBU2jsAAECF1JOMlH0ItHcAAAAAAKgQSV8AAAAAgArR3gEAKJX2DgAAUCH1JMNlHwKVvgAAAAAAFSLpCwAAAABQIdo7AAClmujtHWq1WtlHAACAtw7tHSYElb4AAAAAABUi6QsAAAAAUCGSvgAAAAAAFaKnLwBQmnq9rqcvABxjh8o+AHB8qcf/8EwAKn0BAAAAACpE0hcAAAAAoEK0dwAASjXR2zsAAABHaaTsA6DSFwAAAACgQg5b6fulL33pzToHAADwJrhVdT0AQOVp7wAAlEp7BwAAqJB6kuGyD4H2DgAAAAAAFSLpCwAAAABQIdo7AACl0t4BAAAqRHuHCUGlLwAAAABAhUj6AgAAAABUiPYOAECptHcAAIAKqSc5VPYhUOkLAAAAAFAhkr4AAAAAABWivQMAUJp6va69AwAAVEk9yUjZhziC46AM9jh4iwAAAAAAxw9JXwAAAACACpH0BQAAAACoED19AYBS6ekLAAAVM1z2AY7gxLIPcOyp9AUAAAAAqBBJXwAAAACACtHeAQAolfYOAABQIfVo7zABqPQFAAAAAKgQSV8AAAAAgArR3gEAKJX2DgAAUCH1JIfKPgQqfQEAAAAAKkTSFwAAAACgBXv37k13d3c6OjrS3d2dffv2Nazp7+/Pe9/73ixYsCALFy7MfffdN3rt/e9/fzo7O9PZ2Zm5c+fmqquuSvL6X0B+9rOfTXt7exYuXJj/+T//5+ienp6edHR0pKOjIz09PS2dU9IXAChVvV6f0A8AAOAo1JOMTPDHOKxfvz5Lly7NwMBAli5dmvXr1zesmTZtWu6+++48+eST2bZtWz73uc/lxRdfTJL88Ic/TH9//2hi+Oqrr06SPPjggxkYGMjAwEDuuuuufOpTn0ryepL5tttuyxNPPJHe3t7cdttthYnmf0jSFwAAAACgBVu3bs2qVauSJKtWrcoDDzzQsGb+/Pnp6OhIksydOzezZs3K7t27x6zZv39/fvCDH4xW+m7dujUrV65MrVbLkiVL8uKLL2ZwcDDf+9730t3dnenTp+fMM89Md3d3tm3bdsRzGuQGAAAAABw3du/ena6urtHnq1evzurVq1vaOzQ0lDlz5iRJZs+enaGhocOu7+3tzcGDB3PhhReOiT/wwANZunRpTjvttCTJzp07c84554xenzdvXnbu3Nk0fiSSvgBAabRQAACAiqknGS77EIc3c9bM9PX1Nb1+2WWX5fnnn2+Ir1u3bszzWq2WWq3W9D6Dg4O58cYb09PTkxNOGNtw4d57780tt9xylCdvnaQvAAAAAMD/56GHHmp67eyzz87g4GDmzJmTwcHBzJo1q3Dd/v37c8UVV2TdunVZsmTJmGsvvPBCent78+1vf3s01tbWlmeffXb0+Y4dO9LW1pa2trY8+uijY+KXXnrpEd+Dnr4AAAAAAC1Yvnx5enp6kiQ9PT258sorG9YcPHgwK1asyMqVK3PNNdc0XL///vvzW7/1WznppJPG3Pfuu+9OvV7Pn/3Zn+X000/PnDlzsmzZsmzfvj379u3Lvn37sn379ixbtuyI55T0BQAAAABowZo1a/L9738/HR0deeihh7JmzZokSV9f32i7hi1btuTxxx/Ppk2b0tnZmc7OzvT394/eY/PmzbnhhhvG3PcjH/lILrjggrS3t+d3fud38gd/8AdJkunTp+eLX/xiFi9enMWLF2ft2rWZPn36Ec9Zq2ukBwCU5KSTTsq5555b9jEO69RTTz1svy8AAOD/qZ3YlZw1sX9/XjS3q/K/46v0BQAAAACoEElfAAAAAIAKmVz2AQCA45tOUwAAUCH1JIfKPgQqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUSD3JSNmHQKUvAAAAAECFSPoCAAAAAFSI9g4AQKm0dwAAgAqpJxku+xCo9AUAAAAAqBBJXwAAAACACtHeAQAoTb1e194BAACqRHuHCUGlLwAAAABAhUj6AgAAAABUiKQvAAAAAECF6OkLAJRKT18AAKiQepJDZR8Clb4AAAAAABUi6QsAAAAAUCGSvgBAqer1+oR+jMfevXvT3d2djo6OdHd3Z9++fYXrenp60tHRkY6OjvT09CRJXn755XR2do4+zjrrrHzuc59LkmzatCkzZ84cvbZhw4ZxnRMAAN5QIxP8cRyQ9AUAOEbWr1+fpUuXZmBgIEuXLs369esb1uzduze33XZbnnjiifT29ua2227Lvn37cuqpp6a/v3/0ce655+bqq68e3XfdddeNXrvlllvezLcFAABMcJK+AADHyNatW7Nq1aokyapVq/LAAw80rPne976X7u7uTJ8+PWeeeWa6u7uzbdu2MWv+6q/+Krt27cr73//+N+PYAADAW5ykLwBQqrLbNxzpsXv37nR1dY0+7rrrrpbf29DQUObMmZMkmT17doaGhhrW7Ny5M+ecc87o83nz5mXnzp1j1mzevDnXXXddarXaaOyP//iPs3DhwlxzzTV59tlnj/bLDgAAx059gj+OA5PLPgAAwEQ2c+bM9PX1Nb1+2WWX5fnnn2+Ir1u3bszzWq02Jml7NDZv3pxvfvObo8//6T/9p7nhhhsyderU/Mf/+B+zatWq/OAHP/iV7g0AAFSPpC8AwDg89NBDTa+dffbZGRwczJw5czI4OJhZs2Y1rGlra8ujjz46+nzHjh259NJLR5//+Mc/zvDwcBYtWjQamzFjxui/b7nllvzrf/2vx/cmAACAStHeAQAoVdntG470GI/ly5enp6cnSdLT05Mrr7yyYc2yZcuyffv27Nu3L/v27cv27duzbNmy0ev33ntvbrjhhjF7BgcHR//9ne98J+94xzvGdU4AAKBaVPoCABwja9asybXXXpuNGzfm3HPPzZYtW5IkfX19ufPOO7Nhw4ZMnz49X/ziF7N48eIkydq1azN9+vTRe2zZsiV/+qd/Oua+X//61/Od73wnkydPzvTp07Np06Y37T0BAAATX60+3hIWAIBf0dSpUzN79uyyj3FYR+rpCwAA/D+1WleSif3786JFXZX/HV+lLwBQmjeihQIAAABj6ekLAAAAAFAhkr4AAAAAABWivQMAUCrtHQAAAN5YKn0BAAAAACpE0hcAAAAAoEIkfQEAAAAAKkRPXwCgVHr6AgBAldSTHCr7EMc9lb4AAAAAABUi6QsAAAAAUCHaOwAApdLeAQAAqqSeZLjsQxz3VPoCAAAAAFSIpC8AAAAAQIVo7wAAlEp7BwAAqJJ6kkNlH+K4p9IXAAAAAKBCJH0BAAAAACpEewcAoDT1el17BwAAqJR6kuGyD3HcU+kLAAAAAFAhkr4AAAAAABWivQMAUCrtHQAAoErqSQ6VfYjjnkpfAAAAAIAKkfQFAAAAAKgQSV8AAAAAgArR0xcAKJWevgAAUCV6+k4EKn0BAAAAACpE0hcAAAAAoEK0dwAASqW9AwAAVM1w2Qc47qn0BQAAAACoEElfAAAAAIAK0d4BACiV9g4AAFAl9SSHyj7EcU+lLwAAAABAhUj6AgAAAABUiPYOAEBp6vW69g4AAFAp9STDZR/iuKfSFwAAAACgQiR9AQAAAAAqRHsHAKBU2jsAAECV1JMcKvsQxz2VvgAAAAAAFSLpCwAAAABQIdo7AACl0t4BAACqpJ5kuOxDHPdU+gIAAAAAVIikLwAAAABAhUj6AgAAAABUiJ6+AECp9PQFAIAqqSc5VPYhjnsqfQEAAAAAKkTSFwAAAACgQrR3AABKpb0DAABUST3JcNmHOO6p9AUAAAAAqBBJXwAAAACACtHeAQAoTb1e194BAAAqpZ7kUNmHOO6p9AUAAAAAqBBJXwAAAACACtHeAQAolfYOAABQJfUkw2Uf4rin0hcAAAAAoEIkfQEAAAAAKkR7BwCgVNo7AABAldSTHCr7EMc9lb4AAAAAABUi6QsAAAAAUCGSvgAAAAAAFaKnLwBQKj19AQCgaobLPsBxT6UvAAAAAEAL9u7dm+7u7nR0dKS7uzv79u1rWNPf35/3vve9WbBgQRYuXJj77rtv9Nr73//+dHZ2prOzM3Pnzs1VV12VJLnnnnuycOHCXHLJJXnf+96XH//4x6N7zjvvvFxyySXp7OxMV1dXS+eU9AUAAAAAaMH69euzdOnSDAwMZOnSpVm/fn3DmmnTpuXuu+/Ok08+mW3btuVzn/tcXnzxxSTJD3/4w/T3948mhq+++uokyfnnn5/HHnssf/mXf5kvfvGLWb169Zh7PvLII+nv709fX19L59TeAQAolfYOAABQJfUkh8o+xDGzdevWPProo0mSVatW5dJLL83tt98+Zs38+fNH/z137tzMmjUru3fvzhlnnDEa379/f37wgx/kD//wD5Mk73vf+0avLVmyJDt27BjXOVX6AgAAAAC0YGhoKHPmzEmSzJ49O0NDQ4dd39vbm4MHD+bCCy8cE3/ggQeydOnSnHbaaQ17Nm7cmMsvv3z0ea1Wy4c+9KEsWrQod911V0vnVOkLAAAAABw3du/ePaY37urVq8e0U7jsssvy/PPPN+xbt27dmOe1Wi21Wq3p6wwODubGG29MT09PTjhhbO3tvffem1tuuaVhzyOPPJKNGzfmRz/60WjsRz/6Udra2rJr1650d3fn7W9/ez7wgQ8c9j1K+gIApanX69o7AABApUz89g4zZ848bG/chx56qOm1s88+O4ODg5kzZ04GBwcza9aswnX79+/PFVdckXXr1mXJkiVjrr3wwgvp7e3Nt7/97THxv/iLv8gtt9ySBx98MDNmzBiNt7W1JUlmzZqVFStWpLe394hJX+0dAAAAAABasHz58vT09CRJenp6cuWVVzasOXjwYFasWJGVK1fmmmuuabh+//3357d+67dy0kknjcZ+/vOf5+qrr843v/nNMT2BX3nllbz88suj/96+fXsuvvjiI55T0hcAAAAAoAVr1qzJ97///XR0dOShhx7KmjVrkiR9fX2j7Rq2bNmSxx9/PJs2bUpnZ2c6OzvT398/eo/NmzfnhhtuGHPfL3/5y9mzZ08+/elPp7Ozc7T9xNDQUH7zN38z73rXu/Lrv/7rueKKK/LhD3/4iOes1f1NJQBQkhNOOCFTp04t+xiHtWDBgsP+6RcAAPD/1GoXJPm3ZR/jsBYt+lrlf8dX6QsAAAAAUCGSvgAAAAAAFTK57AMAAMc3naYAAKBK6kkOlX2I455KXwAAAACACpH0BQAAAACoEElfAAAAAIAK0dMXACiVnr4AAFAl9STDZR/iuKfSFwAA+P+3d/+sUWVhHIB/AwNWYpIRNSQiSGwXBCGVhY5hLESbYGwkXfoUkk8g5CNYOEXQQiSFso2F+gkEW8GgooYYyEQFwX8Dd4sFUeJKIJu97pnngVvMmXvOvLe7/Hh5BwCAggh9AQAAAAAKYrwDAFAr4x0AAKAkVZKvdRcx8HT6AgAAAAAUROgLAAAAAFAQ4x0AgNpUVWW8AwAAFKVK0q+7iIGn0xcAAAAAoCBCXwAAAACAghjvAADUyngHAAAoSZXka91FDDydvgAAAAAABRH6AgAAAAAUxHgHAKBWxjsAAEBJqiT9uosYeDp9AQAAAAAKIvQFAAAAACiI8Q4AQK2MdwAAgJJUSb7WXcTA0+kLAAAAAFAQoS8AAAAAQEGEvgAAAAAABRH6AgC1qqrqt752YnNzM1NTUzl27Fimpqby9u3bn9539uzZDA0N5dy5cz+sP3/+PJOTk5mYmMjMzEy+fPmSJPn8+XNmZmYyMTGRycnJvHjxYkd1AgDAv6dK0v/Nr/IJfQEAdsni4mLa7XaePn2adrudxcXFn9535cqV3LhxY8v6wsJC5ufns7KykuHh4XS73SRJt9vN8PBwVlZWMj8/n4WFhV19DgAA4P9F6AsAsEvu3r2b2dnZJMns7Gzu3Lnz0/va7Xb27t37w1pVVXn48GGmp6e37P/+3Onp6Tx48GDHXckAAEA5mnUXAAAMrk6nk42NjbrL+KWPHz/mxIkT3z7Pzc1lbm5uW3vX19czOjqaJDl06FDW19e3/bu9Xi9DQ0NpNv9+XRsfH8/q6mqSZHV1NYcPH06SNJvN7Nu3L71eL/v379/2+QAAsBs6nT+ysfFn3WX80iC8Nwt9AYDa3Lt3r+4SduzMmTN58+bNlvWrV6/+8LnRaKTRaPxXZQEAQC1KeMcvgdAXAGAH7t+//4/fHTx4MGtraxkdHc3a2loOHDiw7XNbrVbevXuXfr+fZrOZ169fZ2xsLEkyNjaWV69eZXx8PP1+P+/fv0+r1drxswAAAGUw0xcAYJecP38+S0tLSZKlpaVcuHBh23sbjUZOnTqV5eXlLfu/P3d5eTmnT5/WRQwAAHzTqPzrBwDAruj1erl48WJevnyZI0eO5Pbt2xkZGcmjR49y7dq1XL9+PUly8uTJPHnyJB8+fEir1Uq3202n08mzZ89y6dKlbG5u5vjx47l582b27NmTT58+5fLly3n8+HFGRkZy69atHD16tOanBQAAfhdCXwAAAACAghjvAAAAAABQEKEvAAAAAEBBhL4AAAAAAAUR+gIAAAAAFEToCwAAAABQEKEvAAAAAEBBhL4AAAAAAAX5CzUJ5UKgL8YdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1800x1080 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the occlusion sensitivity map\n",
    "occ_sens = monai.visualize.OcclusionSensitivity(nn_module=trainedModel, mask_size=12, n_batch=10, stride=12)\n",
    "# Only get a single slice to save time.\n",
    "# For the other dimensions (channel, width, height), use\n",
    "# -1 to use 0 and img.shape[x]-1 for min and max, respectively\n",
    "depth_slice = img.shape[3] // 2\n",
    "occ_sens_b_box = [-1, -1, depth_slice, depth_slice, -1, -1, -1, -1]\n",
    "\n",
    "occ_result, _ = occ_sens(x=img, b_box=occ_sens_b_box)\n",
    "occ_result = occ_result[..., label.argmax().item()]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 15), facecolor=\"white\")\n",
    "\n",
    "for i, im in enumerate([img[:, :, 50, ...], occ_result]):\n",
    "    cmap = \"gray\" if i == 0 else \"jet\"\n",
    "    ax = axes[i]\n",
    "    im_show = ax.imshow(np.squeeze(im[0][0].detach().cpu()), cmap=cmap)\n",
    "    ax.axis(\"off\")\n",
    "    fig.colorbar(im_show, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
